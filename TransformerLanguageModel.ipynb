{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransformerLanguageModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinyingtld/python/blob/main/TransformerLanguageModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKMLvbP1lFey",
        "outputId": "94acd8de-7b4f-4a4a-d819-f5284723c8f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 17 03:15:25 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will mount a Google drive containing the data of WikiText-2"
      ],
      "metadata": {
        "id": "CmLLtZfwS6mo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIwfVjXNSMMh",
        "outputId": "c57f3ed2-e622-46e7-ffb6-53c3d32159ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/NTU_Course/AI6103/Python Examples/wikitext-2\n",
            "total 13M\n",
            "-rw------- 1 root root  159 May 14 15:30 README\n",
            "-rw------- 1 root root 1.2M May 14 15:30 test.txt\n",
            "-rw------- 1 root root  11M May 14 15:30 train.txt\n",
            "-rw------- 1 root root 1.1M May 14 15:30 valid.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# modify the following path accordingly\n",
        "# % cd '/content/drive/MyDrive/Colab Notebooks/datasets/wikitext-2'\n",
        "% cd '/content/drive/My Drive/NTU_Course/AI6103/Python Examples/wikitext-2'\n",
        "\n",
        "% ls -lah\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "metadata": {
        "id": "-f3e_iAZZNmh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the dataset classes: Dictionary for converting between the word and \n",
        "# the dictionary index. Corpus contains all the text. \n",
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids\n"
      ],
      "metadata": {
        "id": "80nzZqkMS5_X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# corpus = Corpus('/content/drive/MyDrive/Colab Notebooks/datasets/wikitext-2')\n",
        "corpus = Corpus('/content/drive/My Drive/NTU_Course/AI6103/Python Examples/wikitext-2')\n",
        "\n",
        "# Find all the ngrams from the text.\n",
        "def find_ngram(data, max_len):\n",
        "    # number of n-grams\n",
        "    num_ngram = data.size(0) // max_len\n",
        "    # target and input\n",
        "    target = data.narrow(0, 1, num_ngram * max_len)\n",
        "    data = data.narrow(0, 0, num_ngram * max_len)\n",
        "\n",
        "    data = data.view(-1, max_len).contiguous()\n",
        "    target = target.view(-1, max_len).contiguous()\n",
        "    return data, target\n",
        "\n",
        "def shuffle(input, target):\n",
        "  # this is synchronized shuffling of both input tokens and targets\n",
        "  assert input.shape[0] == target.shape[0]\n",
        "  p = np.random.permutation(input.shape[0])\n",
        "  return input[p], target[p]\n",
        "\n",
        "\n",
        "\n",
        "# batch_size = 20\n",
        "# max_len = 30\n",
        "# train_data, train_target = find_ngram(corpus.train, max_len)\n",
        "# train_data = train_data.cuda()\n",
        "# train_target = train_target.cuda()\n",
        "# val_data = batchify(corpus.valid, batch_size)\n",
        "# test_data = batchify(corpus.test, batch_size)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "15m97gCWiay4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data, batch_size, i):\n",
        "  index = batch_size * i\n",
        "  return data[index:index+batch_size, :]"
      ],
      "metadata": {
        "id": "wAP_N1fwxqPW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Statistics"
      ],
      "metadata": {
        "id": "w_fzWoM7UDXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus.dictionary.idx2word)\n",
        "# vocabulary size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y2MUbeu0Kn7",
        "outputId": "0c231802-b1ac-41a4-e976-ed085ff89458"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33278"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_dataset(train_data, train_target):\n",
        "  print(train_data[1, :])\n",
        "  print(train_target[1, :])\n",
        "  print(\"training data shape: \", train_data.shape)\n",
        "  print(\"training target shape: \", train_target.shape)\n",
        "  shuffle(train_data, train_target)\n",
        "\n",
        "max_len = 30\n",
        "train_data, train_target = find_ngram(corpus.train, max_len)\n",
        "test_dataset(train_data, train_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQZk4G9Kkpxb",
        "outputId": "18e23c10-faf9-4635-e2b4-ca64092ac4ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([21, 22, 23,  2,  3,  4, 24, 25, 13, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
            "        35, 36, 37, 38, 39, 17, 40, 41, 15, 42, 43, 44])\n",
            "tensor([22, 23,  2,  3,  4, 24, 25, 13, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "        36, 37, 38, 39, 17, 40, 41, 15, 42, 43, 44, 45])\n",
            "training data shape:  torch.Size([69620, 30])\n",
            "training target shape:  torch.Size([69620, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following contains code from https://github.com/hyunwoongko/transformer"
      ],
      "metadata": {
        "id": "xlj-XiyjbiY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-12):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        # '-1' means last dimension. \n",
        "\n",
        "        out = (x - mean) / (std + self.eps)\n",
        "        out = self.gamma * out + self.beta\n",
        "        return out"
      ],
      "metadata": {
        "id": "t1nPDvTwZG65"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaleDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    compute scale dot product attention\n",
        "    Query : given sentence that we focused on (decoder)\n",
        "    Key : every sentence to check relationship with Qeury(encoder)\n",
        "    Value : every sentence same with Key (encoder)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ScaleDotProductAttention, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
        "        # input is 4 dimension tensor\n",
        "        # [batch_size, head, length, d_tensor]\n",
        "        batch_size, head, length, d_tensor = k.size()\n",
        "\n",
        "        # 1. dot product Query with Key^T to compute similarity\n",
        "        k_t = k.transpose(2, 3)  # transpose\n",
        "        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product. \n",
        "        # Since Python 3.5, the @ operator is matmul\n",
        "        # Notice that mm() is a non-broadcastable version of matmul\n",
        "\n",
        "        # 2. apply masking (opt)\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -1e6) \n",
        "            # put a negative number with extremely large magnitude here\n",
        "\n",
        "        # 3. pass them softmax to make [0, 1] range\n",
        "        score = self.softmax(score)\n",
        "\n",
        "        # 4. multiply with Value\n",
        "        v = score @ v\n",
        "\n",
        "        return v, score"
      ],
      "metadata": {
        "id": "fvCPMy3KYvT_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.attention = ScaleDotProductAttention()\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_concat = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        # 1. dot product with weight matrices\n",
        "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
        "\n",
        "        # 2. split tensor by number of heads\n",
        "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
        "\n",
        "        # 3. do scale dot product to compute similarity\n",
        "        out, attention = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        # 4. concat and pass to linear layer\n",
        "        out = self.concat(out)\n",
        "        out = self.w_concat(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def split(self, tensor):\n",
        "        \"\"\"\n",
        "        split tensor by number of head\n",
        "        :param tensor: [batch_size, length, d_model]\n",
        "        :return: [batch_size, head, length, d_tensor]\n",
        "        \"\"\"\n",
        "        batch_size, length, d_model = tensor.size()\n",
        "\n",
        "        d_tensor = d_model // self.n_head\n",
        "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
        "        return tensor\n",
        "\n",
        "    def concat(self, tensor):\n",
        "        \"\"\"\n",
        "        inverse function of self.split(tensor : torch.Tensor)\n",
        "        :param tensor: [batch_size, head, length, d_tensor]\n",
        "        :return: [batch_size, length, d_model]\n",
        "        \"\"\"\n",
        "        batch_size, head, length, d_tensor = tensor.size()\n",
        "        d_model = head * d_tensor\n",
        "\n",
        "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
        "        return tensor"
      ],
      "metadata": {
        "id": "_Q4sZmBMUwdN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8KTFnRs5bKTd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PostionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    compute sinusoid encoding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len, device):\n",
        "        \"\"\"\n",
        "        constructor of sinusoid encoding class\n",
        "        :param d_model: dimension of model\n",
        "        :param max_len: max sequence length\n",
        "        :param device: hardware device setting\n",
        "        \"\"\"\n",
        "        super(PostionalEncoding, self).__init__()\n",
        "\n",
        "        # same size with input matrix (for adding with input matrix)\n",
        "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
        "        self.encoding.requires_grad = False  # we don't need to compute gradient\n",
        "\n",
        "        pos = torch.arange(0, max_len, device=device)\n",
        "        pos = pos.float().unsqueeze(dim=1)\n",
        "        # 1D => 2D unsqueeze to represent word's position\n",
        "\n",
        "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
        "        # 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])\n",
        "        # \"step=2\" means 'i' multiplied with two (same with 2 * i)\n",
        "\n",
        "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
        "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
        "        # compute positional encoding to consider positional information of words\n",
        "\n",
        "    def forward(self, x):\n",
        "        # self.encoding\n",
        "        # [max_len = 512, d_model = 512]\n",
        "\n",
        "        batch_size, seq_len = x.size()\n",
        "        # [batch_size = 128, seq_len = 30]\n",
        "\n",
        "        return self.encoding[:seq_len, :]\n",
        "        # [seq_len = 30, d_model = 512]\n",
        "        # it will add with tok_emb : [128, 30, 512]"
      ],
      "metadata": {
        "id": "Zhy2ZLHwbTHW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Embedding):\n",
        "    \"\"\"\n",
        "    Token Embedding using torch.nn\n",
        "    they will dense representation of word using weighted matrix\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        \"\"\"\n",
        "        class for token embedding that included positional information\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param d_model: dimensions of model\n",
        "        \"\"\"\n",
        "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)"
      ],
      "metadata": {
        "id": "uFRlBMsEbxMn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    token embedding + positional encoding (sinusoid)\n",
        "    positional encoding can give positional information to network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
        "        \"\"\"\n",
        "        class for word embedding that included positional information\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param d_model: dimensions of model\n",
        "        \"\"\"\n",
        "        super(TransformerEmbedding, self).__init__()\n",
        "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
        "        self.pos_emb = PostionalEncoding(d_model, max_len, device)\n",
        "        self.drop_out = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        tok_emb = self.tok_emb(x)\n",
        "        pos_emb = self.pos_emb(x)\n",
        "        return self.drop_out(tok_emb + pos_emb)"
      ],
      "metadata": {
        "id": "XHjZXuXNb6hX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
        "        self.norm1 = LayerNorm(d_model=d_model)\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNorm(d_model=d_model)\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, s_mask):\n",
        "        # LayerNorm + self attention + dropout + skip connection\n",
        "        _x = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.attention(q=x, k=x, v=x, mask=s_mask)       \n",
        "        x = self.dropout1(x) + _x\n",
        "        \n",
        "        # LayerNorm + FFN + dropout + skip connection\n",
        "        _x = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x) + _x\n",
        "        return x"
      ],
      "metadata": {
        "id": "Vh-1yTdIdWYl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
        "        self.norm1 = LayerNorm(d_model=d_model) \n",
        "        # should probably replace with torch.nn.LayerNorm, which is more efficient\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
        "        self.norm2 = LayerNorm(d_model=d_model)\n",
        "        # should probably replace with torch.nn.LayerNorm, which is more efficient\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm3 = LayerNorm(d_model=d_model)\n",
        "        # should probably replace with torch.nn.LayerNorm, which is more efficient\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, dec, enc, t_mask, s_mask):\n",
        "        # 1. compute self attention\n",
        "        x = self.norm1(dec)\n",
        "        x = self.self_attention(q=x, k=x, v=x, mask=t_mask)                \n",
        "        x = self.dropout1(x) + dec\n",
        "\n",
        "        if enc is not None:\n",
        "            # 3. compute encoder - decoder attention\n",
        "            _x = x\n",
        "            x = self.norm2(x)\n",
        "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=s_mask)                       \n",
        "            x = self.dropout2(x) + _x\n",
        "\n",
        "        # 5. LayerNorm + FFN + dropout + skip\n",
        "        _x = x\n",
        "        x = self.norm3(x)\n",
        "        x = self.ffn(x)        \n",
        "        x = self.dropout3(x) + _x\n",
        "        return x"
      ],
      "metadata": {
        "id": "9npEoo_QcCVJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
        "        super().__init__()\n",
        "        self.emb = TransformerEmbedding(d_model=d_model,\n",
        "                                        max_len=max_len,\n",
        "                                        vocab_size=enc_voc_size,\n",
        "                                        drop_prob=drop_prob,\n",
        "                                        device=device)\n",
        "\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n",
        "                                                  ffn_hidden=ffn_hidden,\n",
        "                                                  n_head=n_head,\n",
        "                                                  drop_prob=drop_prob)\n",
        "                                     for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, s_mask):\n",
        "        x = self.emb(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, s_mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "3hjpUUB2gRrw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Standard multi-class classification. \"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "metadata": {
        "id": "H05mbdkJia0b"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLM(nn.Module):\n",
        "\n",
        "    def __init__(self, voc_size, max_len, d_model=32, n_head=2, ffn_hidden=32, n_layers=1, drop_prob=0.1, device=\"cuda\"):\n",
        "        super().__init__()\n",
        "        # self.src_pad_idx = src_pad_idx\n",
        "        # self.trg_pad_idx = trg_pad_idx\n",
        "        # self.trg_sos_idx = trg_sos_idx\n",
        "        self.device = device\n",
        "        self.encoder = Encoder(d_model=d_model,\n",
        "                               n_head=n_head,\n",
        "                               max_len=max_len,\n",
        "                               ffn_hidden=ffn_hidden,\n",
        "                               enc_voc_size=voc_size,\n",
        "                               drop_prob=drop_prob,\n",
        "                               n_layers=n_layers,\n",
        "                               device=device)\n",
        "        self.classifier = Generator(d_model, voc_size)\n",
        "\n",
        "        # self.decoder = Decoder(d_model=d_model,\n",
        "        #                        n_head=n_head,\n",
        "        #                        max_len=max_len,\n",
        "        #                        ffn_hidden=ffn_hidden,\n",
        "        #                        dec_voc_size=dec_voc_size,\n",
        "        #                        drop_prob=drop_prob,\n",
        "        #                        n_layers=n_layers,\n",
        "        #                        device=device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        src_mask = self.make_no_peek_mask(x, x)\n",
        "\n",
        "        # src_trg_mask = self.make_pad_mask(trg, src)\n",
        "\n",
        "        # trg_mask = self.make_pad_mask(trg, trg) * \\\n",
        "        #            self.make_no_peek_mask(trg, trg)\n",
        "\n",
        "        output = self.encoder(x, src_mask)\n",
        "        output = self.classifier(output)\n",
        "        # output = self.decoder(trg, enc_src, trg_mask, src_trg_mask)\n",
        "        return output\n",
        "\n",
        "    # def make_pad_mask(self, q, k):\n",
        "    #     len_q, len_k = q.size(1), k.size(1)\n",
        "\n",
        "    #     # batch_size x 1 x 1 x len_k\n",
        "    #     k = k.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    #     # batch_size x 1 x len_q x len_k\n",
        "    #     k = k.repeat(1, 1, len_q, 1)\n",
        "\n",
        "    #     # batch_size x 1 x len_q x 1\n",
        "    #     q = q.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "    #     # batch_size x 1 x len_q x len_k\n",
        "    #     q = q.repeat(1, 1, 1, len_k)\n",
        "\n",
        "    #     mask = k & q\n",
        "    #     return mask\n",
        "\n",
        "    def make_no_peek_mask(self, q, k):\n",
        "        len_q, len_k = q.size(1), k.size(1)\n",
        "\n",
        "        # len_q x len_k\n",
        "        # torch.tril returns the lower triangular portion of the matrix argument\n",
        "        # here the argument is an all-one matrix. \n",
        "        mask = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
        "\n",
        "        return mask"
      ],
      "metadata": {
        "id": "D45erKtyguGa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "def evaluate(model, val_data, val_target, config):\n",
        "  # Turn on evaluation mode which disables dropout.\n",
        "  model.eval()\n",
        "  total_loss = 0.\n",
        "  \n",
        "  batch_size = config['val_batch_size']\n",
        "  num_data = val_data.shape[0]\n",
        "  num_batch = math.ceil(num_data / batch_size)\n",
        "  voc_size = config['voc_size']\n",
        "  criterion = nn.NLLLoss(reduction=\"sum\")\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for batch in range(0, num_batch):\n",
        "      last_index = min(batch_size*(batch+1), num_data)\n",
        "      # The last index of data that we will take\n",
        "      # This is important because we cannot throw away validation or test data.\n",
        "      # We can throw away training data when convenient.\n",
        "\n",
        "      # get the data from the validation set\n",
        "      input = val_data[batch_size*batch:last_index, :]\n",
        "      target = val_target[batch_size*batch:last_index, :]\n",
        "      output = model(input)\n",
        "      output = output.view(-1, voc_size)\n",
        "\n",
        "      total_loss += criterion(output, target.view(-1)).item()\n",
        "  return total_loss / num_data / config['max_len']\n",
        "\n",
        "\n",
        "\n",
        "def train(model, train_data, train_target, optimizer, scheduler, criterion, config):\n",
        "  # shuffle the dataset. Extremely important!\n",
        "  train_data, train_target = shuffle(train_data, train_target)\n",
        "\n",
        "  model.train()\n",
        "  total_loss = 0.\n",
        "  batch_size = config[\"batch_size\"]\n",
        "  num_batch = train_data.shape[0] // batch_size\n",
        "  start_time = time.time() # record starting time\n",
        "  for batch in range(0, num_batch):\n",
        "    # this is one training iteration\n",
        "    input = get_batch(train_data, batch_size, batch)\n",
        "    targets = get_batch(train_target, batch_size, batch)        \n",
        "    model.zero_grad()\n",
        "    output = model(input).reshape(-1, voc_size)\n",
        "    loss = criterion(output, targets.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (scheduler is not None):\n",
        "      scheduler.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    log_interval = config['log_interval']\n",
        "    if batch % log_interval == 0 and batch > 0:\n",
        "      cur_loss = total_loss / log_interval\n",
        "      elapsed = time.time() - start_time\n",
        "      print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.6f} | ms/batch {:5.2f} | '\n",
        "              'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "          config['epoch'], batch, num_batch, optimizer.param_groups[0]['lr'],\n",
        "          elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "      total_loss = 0\n",
        "      start_time = time.time()\n",
        "\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "  config = {}\n",
        "  config['log_interval'] = 100\n",
        "  config['min_lr'] = 1e-8\n",
        "  config['max_lr'] = 1e-3\n",
        "  config['weight_decay'] = 1e-4\n",
        "  config['batch_size'] = 64\n",
        "  config['max_len'] = 32\n",
        "  config['num_epochs'] = 30\n",
        "  config['voc_size'] = len(corpus.dictionary)\n",
        "  config['val_batch_size'] = 128 \n",
        "  # we can use larger batches during evaluation to save time\n",
        "  # this is because evaluation runs in inference \n",
        "  # mode, which consumes less memory\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "\n",
        "  train_data, train_target = find_ngram(corpus.train, config['max_len'])\n",
        "  train_data = train_data.cuda()\n",
        "  train_target = train_target.cuda()\n",
        "\n",
        "  val_data, val_target = find_ngram(corpus.valid, config['max_len'])\n",
        "  val_data = val_data.cuda()\n",
        "  val_target = val_target.cuda()\n",
        "\n",
        "  test_data, test_target = find_ngram(corpus.test, config['max_len'])\n",
        "  test_data = test_data.cuda()\n",
        "  test_target = test_target.cuda()\n",
        "\n",
        "  voc_size = len(corpus.dictionary)\n",
        "  model = TransformerLM(voc_size, config['max_len'], d_model=256, n_head=16, ffn_hidden=512, n_layers=3, drop_prob=0.2).cuda()\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=config['max_lr'], weight_decay=config['weight_decay'])\n",
        "  total_steps = train_data.shape[0] // config['batch_size'] * config['num_epochs']\n",
        "  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=config['min_lr'])\n",
        "  \n",
        "\n",
        "  for epoch in range(1, config['num_epochs']+1):      \n",
        "    epoch_start_time = time.time()\n",
        "    config['epoch'] = epoch\n",
        "    train(model, train_data, train_target, optimizer, scheduler, criterion, config)\n",
        "\n",
        "    val_loss = evaluate(model, val_data, val_target, config)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                          val_loss, math.exp(val_loss)))\n",
        "    \n",
        "  test_loss = evaluate(model, test_data, test_target, config)\n",
        "  print('| End of training | valid loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
        "\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print('-' * 89)\n",
        "  print('Exiting from training early')"
      ],
      "metadata": {
        "id": "4rty6HHVksGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b1516dc-3308-4ae6-cbd4-2074285e5eb7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   100/ 1019 batches | lr 0.001000 | ms/batch 68.67 | loss  7.71 | ppl  2229.38\n",
            "| epoch   1 |   200/ 1019 batches | lr 0.001000 | ms/batch 66.97 | loss  7.06 | ppl  1161.83\n",
            "| epoch   1 |   300/ 1019 batches | lr 0.001000 | ms/batch 67.45 | loss  6.88 | ppl   975.64\n",
            "| epoch   1 |   400/ 1019 batches | lr 0.001000 | ms/batch 67.33 | loss  6.77 | ppl   870.61\n",
            "| epoch   1 |   500/ 1019 batches | lr 0.000999 | ms/batch 67.57 | loss  6.68 | ppl   799.24\n",
            "| epoch   1 |   600/ 1019 batches | lr 0.000999 | ms/batch 67.84 | loss  6.63 | ppl   759.10\n",
            "| epoch   1 |   700/ 1019 batches | lr 0.000999 | ms/batch 67.89 | loss  6.59 | ppl   725.70\n",
            "| epoch   1 |   800/ 1019 batches | lr 0.000998 | ms/batch 68.21 | loss  6.53 | ppl   683.99\n",
            "| epoch   1 |   900/ 1019 batches | lr 0.000998 | ms/batch 68.40 | loss  6.51 | ppl   668.84\n",
            "| epoch   1 |  1000/ 1019 batches | lr 0.000997 | ms/batch 68.64 | loss  6.46 | ppl   641.75\n",
            "| end of epoch   1 | time: 71.17s | valid loss  5.70 | valid ppl   298.05\n",
            "| epoch   2 |   100/ 1019 batches | lr 0.000997 | ms/batch 69.77 | loss  6.29 | ppl   536.69\n",
            "| epoch   2 |   200/ 1019 batches | lr 0.000996 | ms/batch 70.25 | loss  6.23 | ppl   507.87\n",
            "| epoch   2 |   300/ 1019 batches | lr 0.000995 | ms/batch 69.47 | loss  6.22 | ppl   501.59\n",
            "| epoch   2 |   400/ 1019 batches | lr 0.000995 | ms/batch 70.20 | loss  6.23 | ppl   507.96\n",
            "| epoch   2 |   500/ 1019 batches | lr 0.000994 | ms/batch 69.94 | loss  6.22 | ppl   502.48\n",
            "| epoch   2 |   600/ 1019 batches | lr 0.000993 | ms/batch 69.98 | loss  6.19 | ppl   489.77\n",
            "| epoch   2 |   700/ 1019 batches | lr 0.000992 | ms/batch 70.15 | loss  6.19 | ppl   488.56\n",
            "| epoch   2 |   800/ 1019 batches | lr 0.000991 | ms/batch 70.49 | loss  6.17 | ppl   476.81\n",
            "| epoch   2 |   900/ 1019 batches | lr 0.000990 | ms/batch 70.89 | loss  6.18 | ppl   482.31\n",
            "| epoch   2 |  1000/ 1019 batches | lr 0.000989 | ms/batch 71.04 | loss  6.15 | ppl   470.68\n",
            "| end of epoch   2 | time: 73.62s | valid loss  5.48 | valid ppl   240.44\n",
            "| epoch   3 |   100/ 1019 batches | lr 0.000988 | ms/batch 71.89 | loss  5.98 | ppl   393.98\n",
            "| epoch   3 |   200/ 1019 batches | lr 0.000987 | ms/batch 71.38 | loss  5.94 | ppl   380.47\n",
            "| epoch   3 |   300/ 1019 batches | lr 0.000986 | ms/batch 71.29 | loss  5.94 | ppl   381.42\n",
            "| epoch   3 |   400/ 1019 batches | lr 0.000984 | ms/batch 71.39 | loss  5.96 | ppl   388.67\n",
            "| epoch   3 |   500/ 1019 batches | lr 0.000983 | ms/batch 71.48 | loss  5.98 | ppl   394.11\n",
            "| epoch   3 |   600/ 1019 batches | lr 0.000982 | ms/batch 71.55 | loss  5.97 | ppl   390.76\n",
            "| epoch   3 |   700/ 1019 batches | lr 0.000980 | ms/batch 71.84 | loss  5.98 | ppl   394.70\n",
            "| epoch   3 |   800/ 1019 batches | lr 0.000979 | ms/batch 72.13 | loss  5.97 | ppl   391.92\n",
            "| epoch   3 |   900/ 1019 batches | lr 0.000977 | ms/batch 72.17 | loss  5.98 | ppl   395.62\n",
            "| epoch   3 |  1000/ 1019 batches | lr 0.000976 | ms/batch 72.26 | loss  5.97 | ppl   392.56\n",
            "| end of epoch   3 | time: 75.26s | valid loss  5.38 | valid ppl   216.48\n",
            "| epoch   4 |   100/ 1019 batches | lr 0.000974 | ms/batch 73.28 | loss  5.79 | ppl   327.53\n",
            "| epoch   4 |   200/ 1019 batches | lr 0.000972 | ms/batch 72.66 | loss  5.77 | ppl   318.95\n",
            "| epoch   4 |   300/ 1019 batches | lr 0.000971 | ms/batch 72.62 | loss  5.79 | ppl   325.51\n",
            "| epoch   4 |   400/ 1019 batches | lr 0.000969 | ms/batch 72.59 | loss  5.81 | ppl   334.10\n",
            "| epoch   4 |   500/ 1019 batches | lr 0.000967 | ms/batch 72.73 | loss  5.81 | ppl   334.78\n",
            "| epoch   4 |   600/ 1019 batches | lr 0.000965 | ms/batch 72.85 | loss  5.83 | ppl   339.14\n",
            "| epoch   4 |   700/ 1019 batches | lr 0.000963 | ms/batch 72.91 | loss  5.82 | ppl   337.39\n",
            "| epoch   4 |   800/ 1019 batches | lr 0.000961 | ms/batch 73.11 | loss  5.83 | ppl   339.61\n",
            "| epoch   4 |   900/ 1019 batches | lr 0.000959 | ms/batch 73.10 | loss  5.83 | ppl   339.16\n",
            "| epoch   4 |  1000/ 1019 batches | lr 0.000957 | ms/batch 73.31 | loss  5.83 | ppl   340.67\n",
            "| end of epoch   4 | time: 76.49s | valid loss  5.32 | valid ppl   204.36\n",
            "| epoch   5 |   100/ 1019 batches | lr 0.000955 | ms/batch 74.10 | loss  5.65 | ppl   283.77\n",
            "| epoch   5 |   200/ 1019 batches | lr 0.000952 | ms/batch 73.45 | loss  5.64 | ppl   281.57\n",
            "| epoch   5 |   300/ 1019 batches | lr 0.000950 | ms/batch 73.77 | loss  5.66 | ppl   288.36\n",
            "| epoch   5 |   400/ 1019 batches | lr 0.000948 | ms/batch 74.10 | loss  5.68 | ppl   294.00\n",
            "| epoch   5 |   500/ 1019 batches | lr 0.000946 | ms/batch 74.50 | loss  5.69 | ppl   297.30\n",
            "| epoch   5 |   600/ 1019 batches | lr 0.000943 | ms/batch 74.47 | loss  5.71 | ppl   301.46\n",
            "| epoch   5 |   700/ 1019 batches | lr 0.000941 | ms/batch 74.56 | loss  5.71 | ppl   301.71\n",
            "| epoch   5 |   800/ 1019 batches | lr 0.000939 | ms/batch 74.41 | loss  5.71 | ppl   301.98\n",
            "| epoch   5 |   900/ 1019 batches | lr 0.000936 | ms/batch 74.44 | loss  5.73 | ppl   308.87\n",
            "| epoch   5 |  1000/ 1019 batches | lr 0.000933 | ms/batch 74.36 | loss  5.73 | ppl   308.09\n",
            "| end of epoch   5 | time: 77.84s | valid loss  5.29 | valid ppl   198.80\n",
            "| epoch   6 |   100/ 1019 batches | lr 0.000930 | ms/batch 74.95 | loss  5.55 | ppl   257.53\n",
            "| epoch   6 |   200/ 1019 batches | lr 0.000928 | ms/batch 74.28 | loss  5.55 | ppl   256.82\n",
            "| epoch   6 |   300/ 1019 batches | lr 0.000925 | ms/batch 74.29 | loss  5.57 | ppl   261.36\n",
            "| epoch   6 |   400/ 1019 batches | lr 0.000922 | ms/batch 74.37 | loss  5.58 | ppl   265.02\n",
            "| epoch   6 |   500/ 1019 batches | lr 0.000920 | ms/batch 74.41 | loss  5.61 | ppl   272.45\n",
            "| epoch   6 |   600/ 1019 batches | lr 0.000917 | ms/batch 74.43 | loss  5.60 | ppl   270.22\n",
            "| epoch   6 |   700/ 1019 batches | lr 0.000914 | ms/batch 74.38 | loss  5.61 | ppl   274.16\n",
            "| epoch   6 |   800/ 1019 batches | lr 0.000911 | ms/batch 74.47 | loss  5.63 | ppl   279.19\n",
            "| epoch   6 |   900/ 1019 batches | lr 0.000908 | ms/batch 74.59 | loss  5.64 | ppl   280.42\n",
            "| epoch   6 |  1000/ 1019 batches | lr 0.000905 | ms/batch 74.48 | loss  5.64 | ppl   281.46\n",
            "| end of epoch   6 | time: 78.12s | valid loss  5.26 | valid ppl   193.10\n",
            "| epoch   7 |   100/ 1019 batches | lr 0.000901 | ms/batch 75.23 | loss  5.47 | ppl   237.66\n",
            "| epoch   7 |   200/ 1019 batches | lr 0.000898 | ms/batch 74.53 | loss  5.45 | ppl   233.14\n",
            "| epoch   7 |   300/ 1019 batches | lr 0.000895 | ms/batch 74.49 | loss  5.47 | ppl   238.53\n",
            "| epoch   7 |   400/ 1019 batches | lr 0.000892 | ms/batch 74.51 | loss  5.50 | ppl   245.70\n",
            "| epoch   7 |   500/ 1019 batches | lr 0.000889 | ms/batch 74.51 | loss  5.52 | ppl   250.52\n",
            "| epoch   7 |   600/ 1019 batches | lr 0.000886 | ms/batch 74.53 | loss  5.53 | ppl   251.46\n",
            "| epoch   7 |   700/ 1019 batches | lr 0.000882 | ms/batch 74.64 | loss  5.53 | ppl   252.89\n",
            "| epoch   7 |   800/ 1019 batches | lr 0.000879 | ms/batch 74.59 | loss  5.56 | ppl   258.74\n",
            "| epoch   7 |   900/ 1019 batches | lr 0.000876 | ms/batch 74.56 | loss  5.56 | ppl   259.96\n",
            "| epoch   7 |  1000/ 1019 batches | lr 0.000872 | ms/batch 74.46 | loss  5.58 | ppl   264.57\n",
            "| end of epoch   7 | time: 78.26s | valid loss  5.23 | valid ppl   187.20\n",
            "| epoch   8 |   100/ 1019 batches | lr 0.000868 | ms/batch 75.22 | loss  5.40 | ppl   220.67\n",
            "| epoch   8 |   200/ 1019 batches | lr 0.000865 | ms/batch 74.50 | loss  5.37 | ppl   214.92\n",
            "| epoch   8 |   300/ 1019 batches | lr 0.000861 | ms/batch 74.49 | loss  5.42 | ppl   224.82\n",
            "| epoch   8 |   400/ 1019 batches | lr 0.000857 | ms/batch 74.48 | loss  5.41 | ppl   224.24\n",
            "| epoch   8 |   500/ 1019 batches | lr 0.000854 | ms/batch 74.44 | loss  5.45 | ppl   232.30\n",
            "| epoch   8 |   600/ 1019 batches | lr 0.000850 | ms/batch 74.47 | loss  5.46 | ppl   235.17\n",
            "| epoch   8 |   700/ 1019 batches | lr 0.000847 | ms/batch 74.44 | loss  5.49 | ppl   241.37\n",
            "| epoch   8 |   800/ 1019 batches | lr 0.000843 | ms/batch 74.47 | loss  5.48 | ppl   240.29\n",
            "| epoch   8 |   900/ 1019 batches | lr 0.000839 | ms/batch 74.39 | loss  5.49 | ppl   242.27\n",
            "| epoch   8 |  1000/ 1019 batches | lr 0.000835 | ms/batch 74.42 | loss  5.49 | ppl   243.43\n",
            "| end of epoch   8 | time: 78.19s | valid loss  5.23 | valid ppl   186.78\n",
            "| epoch   9 |   100/ 1019 batches | lr 0.000831 | ms/batch 75.25 | loss  5.34 | ppl   208.23\n",
            "| epoch   9 |   200/ 1019 batches | lr 0.000827 | ms/batch 74.60 | loss  5.32 | ppl   204.49\n",
            "| epoch   9 |   300/ 1019 batches | lr 0.000823 | ms/batch 74.53 | loss  5.34 | ppl   208.02\n",
            "| epoch   9 |   400/ 1019 batches | lr 0.000819 | ms/batch 74.64 | loss  5.36 | ppl   212.97\n",
            "| epoch   9 |   500/ 1019 batches | lr 0.000815 | ms/batch 74.51 | loss  5.38 | ppl   216.35\n",
            "| epoch   9 |   600/ 1019 batches | lr 0.000811 | ms/batch 74.50 | loss  5.40 | ppl   221.71\n",
            "| epoch   9 |   700/ 1019 batches | lr 0.000807 | ms/batch 74.50 | loss  5.40 | ppl   222.15\n",
            "| epoch   9 |   800/ 1019 batches | lr 0.000803 | ms/batch 74.52 | loss  5.42 | ppl   226.90\n",
            "| epoch   9 |   900/ 1019 batches | lr 0.000799 | ms/batch 74.54 | loss  5.42 | ppl   225.19\n",
            "| epoch   9 |  1000/ 1019 batches | lr 0.000795 | ms/batch 74.52 | loss  5.44 | ppl   230.14\n",
            "| end of epoch   9 | time: 78.25s | valid loss  5.20 | valid ppl   181.42\n",
            "| epoch  10 |   100/ 1019 batches | lr 0.000790 | ms/batch 75.18 | loss  5.27 | ppl   194.66\n",
            "| epoch  10 |   200/ 1019 batches | lr 0.000785 | ms/batch 74.43 | loss  5.24 | ppl   188.41\n",
            "| epoch  10 |   300/ 1019 batches | lr 0.000781 | ms/batch 74.43 | loss  5.30 | ppl   199.58\n",
            "| epoch  10 |   400/ 1019 batches | lr 0.000777 | ms/batch 74.33 | loss  5.31 | ppl   202.05\n",
            "| epoch  10 |   500/ 1019 batches | lr 0.000773 | ms/batch 74.50 | loss  5.32 | ppl   204.75\n",
            "| epoch  10 |   600/ 1019 batches | lr 0.000768 | ms/batch 74.50 | loss  5.32 | ppl   204.86\n",
            "| epoch  10 |   700/ 1019 batches | lr 0.000764 | ms/batch 74.42 | loss  5.35 | ppl   210.79\n",
            "| epoch  10 |   800/ 1019 batches | lr 0.000760 | ms/batch 74.50 | loss  5.37 | ppl   214.21\n",
            "| epoch  10 |   900/ 1019 batches | lr 0.000755 | ms/batch 74.53 | loss  5.35 | ppl   211.22\n",
            "| epoch  10 |  1000/ 1019 batches | lr 0.000751 | ms/batch 74.53 | loss  5.39 | ppl   219.97\n",
            "| end of epoch  10 | time: 78.19s | valid loss  5.19 | valid ppl   180.13\n",
            "| epoch  11 |   100/ 1019 batches | lr 0.000745 | ms/batch 75.26 | loss  5.21 | ppl   182.74\n",
            "| epoch  11 |   200/ 1019 batches | lr 0.000741 | ms/batch 74.63 | loss  5.20 | ppl   181.53\n",
            "| epoch  11 |   300/ 1019 batches | lr 0.000736 | ms/batch 74.61 | loss  5.24 | ppl   188.04\n",
            "| epoch  11 |   400/ 1019 batches | lr 0.000732 | ms/batch 74.55 | loss  5.25 | ppl   190.82\n",
            "| epoch  11 |   500/ 1019 batches | lr 0.000727 | ms/batch 74.54 | loss  5.27 | ppl   194.97\n",
            "| epoch  11 |   600/ 1019 batches | lr 0.000723 | ms/batch 74.53 | loss  5.28 | ppl   196.12\n",
            "| epoch  11 |   700/ 1019 batches | lr 0.000718 | ms/batch 74.46 | loss  5.29 | ppl   198.62\n",
            "| epoch  11 |   800/ 1019 batches | lr 0.000714 | ms/batch 74.46 | loss  5.30 | ppl   199.66\n",
            "| epoch  11 |   900/ 1019 batches | lr 0.000709 | ms/batch 74.50 | loss  5.32 | ppl   203.58\n",
            "| epoch  11 |  1000/ 1019 batches | lr 0.000704 | ms/batch 74.50 | loss  5.32 | ppl   203.86\n",
            "| end of epoch  11 | time: 78.25s | valid loss  5.19 | valid ppl   179.99\n",
            "| epoch  12 |   100/ 1019 batches | lr 0.000699 | ms/batch 75.13 | loss  5.16 | ppl   173.34\n",
            "| epoch  12 |   200/ 1019 batches | lr 0.000694 | ms/batch 74.48 | loss  5.16 | ppl   173.38\n",
            "| epoch  12 |   300/ 1019 batches | lr 0.000689 | ms/batch 74.51 | loss  5.18 | ppl   176.88\n",
            "| epoch  12 |   400/ 1019 batches | lr 0.000684 | ms/batch 74.40 | loss  5.19 | ppl   180.22\n",
            "| epoch  12 |   500/ 1019 batches | lr 0.000680 | ms/batch 74.49 | loss  5.23 | ppl   186.63\n",
            "| epoch  12 |   600/ 1019 batches | lr 0.000675 | ms/batch 74.48 | loss  5.21 | ppl   183.81\n",
            "| epoch  12 |   700/ 1019 batches | lr 0.000670 | ms/batch 74.44 | loss  5.24 | ppl   188.77\n",
            "| epoch  12 |   800/ 1019 batches | lr 0.000665 | ms/batch 74.37 | loss  5.26 | ppl   192.37\n",
            "| epoch  12 |   900/ 1019 batches | lr 0.000660 | ms/batch 74.40 | loss  5.27 | ppl   193.58\n",
            "| epoch  12 |  1000/ 1019 batches | lr 0.000655 | ms/batch 74.39 | loss  5.29 | ppl   197.82\n",
            "| end of epoch  12 | time: 78.15s | valid loss  5.18 | valid ppl   177.76\n",
            "| epoch  13 |   100/ 1019 batches | lr 0.000650 | ms/batch 75.05 | loss  5.13 | ppl   168.19\n",
            "| epoch  13 |   200/ 1019 batches | lr 0.000645 | ms/batch 74.45 | loss  5.10 | ppl   163.44\n",
            "| epoch  13 |   300/ 1019 batches | lr 0.000640 | ms/batch 74.36 | loss  5.11 | ppl   166.43\n",
            "| epoch  13 |   400/ 1019 batches | lr 0.000635 | ms/batch 74.28 | loss  5.15 | ppl   173.16\n",
            "| epoch  13 |   500/ 1019 batches | lr 0.000630 | ms/batch 74.37 | loss  5.17 | ppl   175.19\n",
            "| epoch  13 |   600/ 1019 batches | lr 0.000625 | ms/batch 74.45 | loss  5.18 | ppl   177.06\n",
            "| epoch  13 |   700/ 1019 batches | lr 0.000620 | ms/batch 74.41 | loss  5.20 | ppl   180.89\n",
            "| epoch  13 |   800/ 1019 batches | lr 0.000615 | ms/batch 74.48 | loss  5.20 | ppl   181.69\n",
            "| epoch  13 |   900/ 1019 batches | lr 0.000610 | ms/batch 74.43 | loss  5.23 | ppl   186.13\n",
            "| epoch  13 |  1000/ 1019 batches | lr 0.000605 | ms/batch 74.30 | loss  5.23 | ppl   187.70\n",
            "| end of epoch  13 | time: 78.09s | valid loss  5.17 | valid ppl   176.68\n",
            "| epoch  14 |   100/ 1019 batches | lr 0.000599 | ms/batch 75.06 | loss  5.08 | ppl   160.65\n",
            "| epoch  14 |   200/ 1019 batches | lr 0.000594 | ms/batch 74.28 | loss  5.07 | ppl   158.56\n",
            "| epoch  14 |   300/ 1019 batches | lr 0.000589 | ms/batch 74.40 | loss  5.09 | ppl   161.68\n",
            "| epoch  14 |   400/ 1019 batches | lr 0.000584 | ms/batch 74.32 | loss  5.10 | ppl   164.13\n",
            "| epoch  14 |   500/ 1019 batches | lr 0.000579 | ms/batch 74.48 | loss  5.11 | ppl   165.78\n",
            "| epoch  14 |   600/ 1019 batches | lr 0.000574 | ms/batch 74.40 | loss  5.14 | ppl   171.18\n",
            "| epoch  14 |   700/ 1019 batches | lr 0.000568 | ms/batch 74.39 | loss  5.15 | ppl   172.90\n",
            "| epoch  14 |   800/ 1019 batches | lr 0.000563 | ms/batch 74.46 | loss  5.15 | ppl   172.93\n",
            "| epoch  14 |   900/ 1019 batches | lr 0.000558 | ms/batch 74.47 | loss  5.17 | ppl   175.41\n",
            "| epoch  14 |  1000/ 1019 batches | lr 0.000553 | ms/batch 74.53 | loss  5.18 | ppl   178.45\n",
            "| end of epoch  14 | time: 78.13s | valid loss  5.17 | valid ppl   176.41\n",
            "| epoch  15 |   100/ 1019 batches | lr 0.000547 | ms/batch 75.19 | loss  5.04 | ppl   154.78\n",
            "| epoch  15 |   200/ 1019 batches | lr 0.000542 | ms/batch 74.50 | loss  5.01 | ppl   150.00\n",
            "| epoch  15 |   300/ 1019 batches | lr 0.000537 | ms/batch 74.48 | loss  5.04 | ppl   154.79\n",
            "| epoch  15 |   400/ 1019 batches | lr 0.000532 | ms/batch 74.46 | loss  5.07 | ppl   159.00\n",
            "| epoch  15 |   500/ 1019 batches | lr 0.000527 | ms/batch 74.54 | loss  5.09 | ppl   161.85\n",
            "| epoch  15 |   600/ 1019 batches | lr 0.000521 | ms/batch 74.47 | loss  5.10 | ppl   163.48\n",
            "| epoch  15 |   700/ 1019 batches | lr 0.000516 | ms/batch 74.48 | loss  5.11 | ppl   165.75\n",
            "| epoch  15 |   800/ 1019 batches | lr 0.000511 | ms/batch 74.45 | loss  5.11 | ppl   165.35\n",
            "| epoch  15 |   900/ 1019 batches | lr 0.000506 | ms/batch 74.47 | loss  5.13 | ppl   169.38\n",
            "| epoch  15 |  1000/ 1019 batches | lr 0.000501 | ms/batch 74.44 | loss  5.13 | ppl   169.03\n",
            "| end of epoch  15 | time: 78.20s | valid loss  5.17 | valid ppl   175.18\n",
            "| epoch  16 |   100/ 1019 batches | lr 0.000495 | ms/batch 75.17 | loss  5.01 | ppl   150.63\n",
            "| epoch  16 |   200/ 1019 batches | lr 0.000490 | ms/batch 74.46 | loss  4.98 | ppl   145.43\n",
            "| epoch  16 |   300/ 1019 batches | lr 0.000485 | ms/batch 74.38 | loss  5.01 | ppl   149.40\n",
            "| epoch  16 |   400/ 1019 batches | lr 0.000479 | ms/batch 74.47 | loss  5.01 | ppl   150.05\n",
            "| epoch  16 |   500/ 1019 batches | lr 0.000474 | ms/batch 74.34 | loss  5.04 | ppl   155.07\n",
            "| epoch  16 |   600/ 1019 batches | lr 0.000469 | ms/batch 74.38 | loss  5.05 | ppl   156.80\n",
            "| epoch  16 |   700/ 1019 batches | lr 0.000464 | ms/batch 74.35 | loss  5.06 | ppl   157.28\n",
            "| epoch  16 |   800/ 1019 batches | lr 0.000459 | ms/batch 74.31 | loss  5.07 | ppl   159.77\n",
            "| epoch  16 |   900/ 1019 batches | lr 0.000454 | ms/batch 74.29 | loss  5.09 | ppl   161.87\n",
            "| epoch  16 |  1000/ 1019 batches | lr 0.000449 | ms/batch 74.24 | loss  5.11 | ppl   165.72\n",
            "| end of epoch  16 | time: 78.08s | valid loss  5.17 | valid ppl   176.33\n",
            "| epoch  17 |   100/ 1019 batches | lr 0.000443 | ms/batch 75.05 | loss  4.98 | ppl   144.93\n",
            "| epoch  17 |   200/ 1019 batches | lr 0.000437 | ms/batch 74.39 | loss  4.94 | ppl   139.77\n",
            "| epoch  17 |   300/ 1019 batches | lr 0.000432 | ms/batch 74.41 | loss  4.97 | ppl   144.65\n",
            "| epoch  17 |   400/ 1019 batches | lr 0.000427 | ms/batch 74.44 | loss  4.98 | ppl   145.61\n",
            "| epoch  17 |   500/ 1019 batches | lr 0.000422 | ms/batch 74.37 | loss  5.00 | ppl   149.04\n",
            "| epoch  17 |   600/ 1019 batches | lr 0.000417 | ms/batch 74.44 | loss  5.03 | ppl   152.76\n",
            "| epoch  17 |   700/ 1019 batches | lr 0.000412 | ms/batch 74.33 | loss  5.03 | ppl   152.89\n",
            "| epoch  17 |   800/ 1019 batches | lr 0.000407 | ms/batch 74.29 | loss  5.03 | ppl   152.24\n",
            "| epoch  17 |   900/ 1019 batches | lr 0.000402 | ms/batch 74.36 | loss  5.05 | ppl   155.90\n",
            "| epoch  17 |  1000/ 1019 batches | lr 0.000397 | ms/batch 74.31 | loss  5.07 | ppl   159.35\n",
            "| end of epoch  17 | time: 78.08s | valid loss  5.16 | valid ppl   174.44\n",
            "| epoch  18 |   100/ 1019 batches | lr 0.000391 | ms/batch 75.09 | loss  4.95 | ppl   141.79\n",
            "| epoch  18 |   200/ 1019 batches | lr 0.000386 | ms/batch 74.33 | loss  4.92 | ppl   136.85\n",
            "| epoch  18 |   300/ 1019 batches | lr 0.000381 | ms/batch 74.51 | loss  4.94 | ppl   139.80\n",
            "| epoch  18 |   400/ 1019 batches | lr 0.000376 | ms/batch 74.49 | loss  4.96 | ppl   142.78\n",
            "| epoch  18 |   500/ 1019 batches | lr 0.000371 | ms/batch 74.34 | loss  4.96 | ppl   142.80\n",
            "| epoch  18 |   600/ 1019 batches | lr 0.000366 | ms/batch 74.52 | loss  4.97 | ppl   143.89\n",
            "| epoch  18 |   700/ 1019 batches | lr 0.000361 | ms/batch 74.41 | loss  5.00 | ppl   149.09\n",
            "| epoch  18 |   800/ 1019 batches | lr 0.000356 | ms/batch 74.40 | loss  4.99 | ppl   147.37\n",
            "| epoch  18 |   900/ 1019 batches | lr 0.000351 | ms/batch 74.38 | loss  5.02 | ppl   150.75\n",
            "| epoch  18 |  1000/ 1019 batches | lr 0.000346 | ms/batch 74.37 | loss  5.01 | ppl   150.56\n",
            "| end of epoch  18 | time: 78.13s | valid loss  5.17 | valid ppl   176.40\n",
            "| epoch  19 |   100/ 1019 batches | lr 0.000341 | ms/batch 75.09 | loss  4.92 | ppl   137.38\n",
            "| epoch  19 |   200/ 1019 batches | lr 0.000336 | ms/batch 74.48 | loss  4.90 | ppl   133.82\n",
            "| epoch  19 |   300/ 1019 batches | lr 0.000331 | ms/batch 74.37 | loss  4.92 | ppl   137.05\n",
            "| epoch  19 |   400/ 1019 batches | lr 0.000326 | ms/batch 74.57 | loss  4.93 | ppl   138.24\n",
            "| epoch  19 |   500/ 1019 batches | lr 0.000321 | ms/batch 74.49 | loss  4.93 | ppl   138.23\n",
            "| epoch  19 |   600/ 1019 batches | lr 0.000316 | ms/batch 74.47 | loss  4.93 | ppl   139.00\n",
            "| epoch  19 |   700/ 1019 batches | lr 0.000312 | ms/batch 74.50 | loss  4.95 | ppl   141.60\n",
            "| epoch  19 |   800/ 1019 batches | lr 0.000307 | ms/batch 74.46 | loss  4.97 | ppl   144.33\n",
            "| epoch  19 |   900/ 1019 batches | lr 0.000302 | ms/batch 74.53 | loss  4.98 | ppl   144.95\n",
            "| epoch  19 |  1000/ 1019 batches | lr 0.000297 | ms/batch 74.56 | loss  4.97 | ppl   144.34\n",
            "| end of epoch  19 | time: 78.20s | valid loss  5.18 | valid ppl   177.03\n",
            "| epoch  20 |   100/ 1019 batches | lr 0.000292 | ms/batch 75.24 | loss  4.90 | ppl   134.46\n",
            "| epoch  20 |   200/ 1019 batches | lr 0.000287 | ms/batch 74.55 | loss  4.87 | ppl   129.88\n",
            "| epoch  20 |   300/ 1019 batches | lr 0.000283 | ms/batch 74.44 | loss  4.88 | ppl   131.04\n",
            "| epoch  20 |   400/ 1019 batches | lr 0.000278 | ms/batch 74.46 | loss  4.90 | ppl   134.08\n",
            "| epoch  20 |   500/ 1019 batches | lr 0.000273 | ms/batch 74.46 | loss  4.89 | ppl   133.51\n",
            "| epoch  20 |   600/ 1019 batches | lr 0.000269 | ms/batch 74.51 | loss  4.92 | ppl   137.33\n",
            "| epoch  20 |   700/ 1019 batches | lr 0.000264 | ms/batch 74.50 | loss  4.91 | ppl   136.12\n",
            "| epoch  20 |   800/ 1019 batches | lr 0.000260 | ms/batch 74.50 | loss  4.94 | ppl   139.19\n",
            "| epoch  20 |   900/ 1019 batches | lr 0.000255 | ms/batch 74.54 | loss  4.94 | ppl   139.96\n",
            "| epoch  20 |  1000/ 1019 batches | lr 0.000251 | ms/batch 74.57 | loss  4.96 | ppl   142.30\n",
            "| end of epoch  20 | time: 78.23s | valid loss  5.17 | valid ppl   176.06\n",
            "| epoch  21 |   100/ 1019 batches | lr 0.000246 | ms/batch 75.20 | loss  4.89 | ppl   132.47\n",
            "| epoch  21 |   200/ 1019 batches | lr 0.000241 | ms/batch 74.52 | loss  4.84 | ppl   127.07\n",
            "| epoch  21 |   300/ 1019 batches | lr 0.000237 | ms/batch 74.49 | loss  4.86 | ppl   128.39\n",
            "| epoch  21 |   400/ 1019 batches | lr 0.000232 | ms/batch 74.46 | loss  4.86 | ppl   129.08\n",
            "| epoch  21 |   500/ 1019 batches | lr 0.000228 | ms/batch 74.58 | loss  4.89 | ppl   132.56\n",
            "| epoch  21 |   600/ 1019 batches | lr 0.000224 | ms/batch 74.46 | loss  4.88 | ppl   131.98\n",
            "| epoch  21 |   700/ 1019 batches | lr 0.000219 | ms/batch 74.64 | loss  4.89 | ppl   132.44\n",
            "| epoch  21 |   800/ 1019 batches | lr 0.000215 | ms/batch 74.46 | loss  4.91 | ppl   135.36\n",
            "| epoch  21 |   900/ 1019 batches | lr 0.000211 | ms/batch 74.47 | loss  4.92 | ppl   136.90\n",
            "| epoch  21 |  1000/ 1019 batches | lr 0.000207 | ms/batch 74.54 | loss  4.92 | ppl   136.79\n",
            "| end of epoch  21 | time: 78.23s | valid loss  5.17 | valid ppl   176.06\n",
            "| epoch  22 |   100/ 1019 batches | lr 0.000202 | ms/batch 75.18 | loss  4.87 | ppl   129.91\n",
            "| epoch  22 |   200/ 1019 batches | lr 0.000198 | ms/batch 74.44 | loss  4.83 | ppl   124.95\n",
            "| epoch  22 |   300/ 1019 batches | lr 0.000194 | ms/batch 74.53 | loss  4.84 | ppl   127.04\n",
            "| epoch  22 |   400/ 1019 batches | lr 0.000190 | ms/batch 74.50 | loss  4.86 | ppl   128.69\n",
            "| epoch  22 |   500/ 1019 batches | lr 0.000186 | ms/batch 74.48 | loss  4.86 | ppl   129.27\n",
            "| epoch  22 |   600/ 1019 batches | lr 0.000182 | ms/batch 74.50 | loss  4.86 | ppl   129.26\n",
            "| epoch  22 |   700/ 1019 batches | lr 0.000178 | ms/batch 74.52 | loss  4.86 | ppl   129.47\n",
            "| epoch  22 |   800/ 1019 batches | lr 0.000174 | ms/batch 74.43 | loss  4.87 | ppl   130.87\n",
            "| epoch  22 |   900/ 1019 batches | lr 0.000170 | ms/batch 74.42 | loss  4.87 | ppl   130.63\n",
            "| epoch  22 |  1000/ 1019 batches | lr 0.000166 | ms/batch 74.57 | loss  4.87 | ppl   130.90\n",
            "| end of epoch  22 | time: 78.21s | valid loss  5.17 | valid ppl   176.76\n",
            "| epoch  23 |   100/ 1019 batches | lr 0.000162 | ms/batch 75.19 | loss  4.85 | ppl   127.89\n",
            "| epoch  23 |   200/ 1019 batches | lr 0.000158 | ms/batch 74.50 | loss  4.80 | ppl   121.23\n",
            "| epoch  23 |   300/ 1019 batches | lr 0.000154 | ms/batch 74.53 | loss  4.82 | ppl   123.94\n",
            "| epoch  23 |   400/ 1019 batches | lr 0.000150 | ms/batch 74.55 | loss  4.84 | ppl   126.49\n",
            "| epoch  23 |   500/ 1019 batches | lr 0.000147 | ms/batch 74.47 | loss  4.83 | ppl   124.99\n",
            "| epoch  23 |   600/ 1019 batches | lr 0.000143 | ms/batch 74.45 | loss  4.84 | ppl   127.01\n",
            "| epoch  23 |   700/ 1019 batches | lr 0.000140 | ms/batch 74.50 | loss  4.85 | ppl   127.43\n",
            "| epoch  23 |   800/ 1019 batches | lr 0.000136 | ms/batch 74.52 | loss  4.84 | ppl   126.00\n",
            "| epoch  23 |   900/ 1019 batches | lr 0.000133 | ms/batch 74.50 | loss  4.87 | ppl   130.03\n",
            "| epoch  23 |  1000/ 1019 batches | lr 0.000129 | ms/batch 74.44 | loss  4.86 | ppl   129.11\n",
            "| end of epoch  23 | time: 78.21s | valid loss  5.17 | valid ppl   176.59\n",
            "| epoch  24 |   100/ 1019 batches | lr 0.000125 | ms/batch 75.24 | loss  4.81 | ppl   123.00\n",
            "| epoch  24 |   200/ 1019 batches | lr 0.000122 | ms/batch 74.55 | loss  4.81 | ppl   123.23\n",
            "| epoch  24 |   300/ 1019 batches | lr 0.000118 | ms/batch 74.52 | loss  4.81 | ppl   122.51\n",
            "| epoch  24 |   400/ 1019 batches | lr 0.000115 | ms/batch 74.39 | loss  4.81 | ppl   122.28\n",
            "| epoch  24 |   500/ 1019 batches | lr 0.000112 | ms/batch 74.51 | loss  4.82 | ppl   124.25\n",
            "| epoch  24 |   600/ 1019 batches | lr 0.000108 | ms/batch 74.44 | loss  4.82 | ppl   124.45\n",
            "| epoch  24 |   700/ 1019 batches | lr 0.000105 | ms/batch 74.40 | loss  4.83 | ppl   124.59\n",
            "| epoch  24 |   800/ 1019 batches | lr 0.000102 | ms/batch 74.50 | loss  4.84 | ppl   126.16\n",
            "| epoch  24 |   900/ 1019 batches | lr 0.000099 | ms/batch 74.50 | loss  4.84 | ppl   126.31\n",
            "| epoch  24 |  1000/ 1019 batches | lr 0.000096 | ms/batch 74.55 | loss  4.84 | ppl   126.12\n",
            "| end of epoch  24 | time: 78.22s | valid loss  5.18 | valid ppl   177.16\n",
            "| epoch  25 |   100/ 1019 batches | lr 0.000092 | ms/batch 75.24 | loss  4.83 | ppl   124.67\n",
            "| epoch  25 |   200/ 1019 batches | lr 0.000090 | ms/batch 74.50 | loss  4.79 | ppl   119.91\n",
            "| epoch  25 |   300/ 1019 batches | lr 0.000087 | ms/batch 74.42 | loss  4.79 | ppl   120.60\n",
            "| epoch  25 |   400/ 1019 batches | lr 0.000084 | ms/batch 74.49 | loss  4.80 | ppl   121.51\n",
            "| epoch  25 |   500/ 1019 batches | lr 0.000081 | ms/batch 74.44 | loss  4.79 | ppl   120.82\n",
            "| epoch  25 |   600/ 1019 batches | lr 0.000078 | ms/batch 74.37 | loss  4.80 | ppl   122.03\n",
            "| epoch  25 |   700/ 1019 batches | lr 0.000075 | ms/batch 74.49 | loss  4.81 | ppl   122.46\n",
            "| epoch  25 |   800/ 1019 batches | lr 0.000073 | ms/batch 74.52 | loss  4.81 | ppl   123.31\n",
            "| epoch  25 |   900/ 1019 batches | lr 0.000070 | ms/batch 74.61 | loss  4.81 | ppl   122.63\n",
            "| epoch  25 |  1000/ 1019 batches | lr 0.000067 | ms/batch 74.46 | loss  4.81 | ppl   123.31\n",
            "| end of epoch  25 | time: 78.21s | valid loss  5.18 | valid ppl   177.24\n",
            "| epoch  26 |   100/ 1019 batches | lr 0.000064 | ms/batch 75.21 | loss  4.82 | ppl   124.29\n",
            "| epoch  26 |   200/ 1019 batches | lr 0.000062 | ms/batch 74.48 | loss  4.77 | ppl   118.19\n",
            "| epoch  26 |   300/ 1019 batches | lr 0.000059 | ms/batch 74.50 | loss  4.78 | ppl   118.81\n",
            "| epoch  26 |   400/ 1019 batches | lr 0.000057 | ms/batch 74.50 | loss  4.78 | ppl   119.68\n",
            "| epoch  26 |   500/ 1019 batches | lr 0.000055 | ms/batch 74.54 | loss  4.78 | ppl   118.56\n",
            "| epoch  26 |   600/ 1019 batches | lr 0.000052 | ms/batch 74.51 | loss  4.79 | ppl   120.53\n",
            "| epoch  26 |   700/ 1019 batches | lr 0.000050 | ms/batch 74.51 | loss  4.80 | ppl   121.50\n",
            "| epoch  26 |   800/ 1019 batches | lr 0.000048 | ms/batch 74.53 | loss  4.80 | ppl   121.32\n",
            "| epoch  26 |   900/ 1019 batches | lr 0.000046 | ms/batch 74.44 | loss  4.80 | ppl   121.81\n",
            "| epoch  26 |  1000/ 1019 batches | lr 0.000044 | ms/batch 74.51 | loss  4.79 | ppl   120.80\n",
            "| end of epoch  26 | time: 78.22s | valid loss  5.18 | valid ppl   177.19\n",
            "| epoch  27 |   100/ 1019 batches | lr 0.000041 | ms/batch 75.23 | loss  4.82 | ppl   124.04\n",
            "| epoch  27 |   200/ 1019 batches | lr 0.000039 | ms/batch 74.46 | loss  4.78 | ppl   119.25\n",
            "| epoch  27 |   300/ 1019 batches | lr 0.000037 | ms/batch 74.41 | loss  4.78 | ppl   119.21\n",
            "| epoch  27 |   400/ 1019 batches | lr 0.000035 | ms/batch 74.40 | loss  4.77 | ppl   117.98\n",
            "| epoch  27 |   500/ 1019 batches | lr 0.000033 | ms/batch 74.46 | loss  4.78 | ppl   118.67\n",
            "| epoch  27 |   600/ 1019 batches | lr 0.000032 | ms/batch 74.49 | loss  4.77 | ppl   118.17\n",
            "| epoch  27 |   700/ 1019 batches | lr 0.000030 | ms/batch 74.40 | loss  4.77 | ppl   118.37\n",
            "| epoch  27 |   800/ 1019 batches | lr 0.000028 | ms/batch 74.53 | loss  4.79 | ppl   119.93\n",
            "| epoch  27 |   900/ 1019 batches | lr 0.000026 | ms/batch 74.48 | loss  4.78 | ppl   118.80\n",
            "| epoch  27 |  1000/ 1019 batches | lr 0.000025 | ms/batch 74.51 | loss  4.78 | ppl   119.08\n",
            "| end of epoch  27 | time: 78.18s | valid loss  5.18 | valid ppl   177.44\n",
            "| epoch  28 |   100/ 1019 batches | lr 0.000023 | ms/batch 75.14 | loss  4.83 | ppl   125.07\n",
            "| epoch  28 |   200/ 1019 batches | lr 0.000021 | ms/batch 74.49 | loss  4.77 | ppl   117.86\n",
            "| epoch  28 |   300/ 1019 batches | lr 0.000020 | ms/batch 74.49 | loss  4.76 | ppl   117.20\n",
            "| epoch  28 |   400/ 1019 batches | lr 0.000019 | ms/batch 74.50 | loss  4.77 | ppl   117.89\n",
            "| epoch  28 |   500/ 1019 batches | lr 0.000017 | ms/batch 74.41 | loss  4.77 | ppl   117.83\n",
            "| epoch  28 |   600/ 1019 batches | lr 0.000016 | ms/batch 74.42 | loss  4.76 | ppl   116.77\n",
            "| epoch  28 |   700/ 1019 batches | lr 0.000015 | ms/batch 74.43 | loss  4.77 | ppl   117.43\n",
            "| epoch  28 |   800/ 1019 batches | lr 0.000013 | ms/batch 74.37 | loss  4.77 | ppl   118.18\n",
            "| epoch  28 |   900/ 1019 batches | lr 0.000012 | ms/batch 74.48 | loss  4.77 | ppl   117.88\n",
            "| epoch  28 |  1000/ 1019 batches | lr 0.000011 | ms/batch 74.50 | loss  4.78 | ppl   118.73\n",
            "| end of epoch  28 | time: 78.17s | valid loss  5.18 | valid ppl   177.58\n",
            "| epoch  29 |   100/ 1019 batches | lr 0.000010 | ms/batch 75.15 | loss  4.82 | ppl   124.53\n",
            "| epoch  29 |   200/ 1019 batches | lr 0.000009 | ms/batch 74.51 | loss  4.75 | ppl   115.86\n",
            "| epoch  29 |   300/ 1019 batches | lr 0.000008 | ms/batch 74.43 | loss  4.77 | ppl   118.13\n",
            "| epoch  29 |   400/ 1019 batches | lr 0.000007 | ms/batch 74.28 | loss  4.76 | ppl   116.85\n",
            "| epoch  29 |   500/ 1019 batches | lr 0.000006 | ms/batch 74.45 | loss  4.76 | ppl   117.16\n",
            "| epoch  29 |   600/ 1019 batches | lr 0.000005 | ms/batch 74.45 | loss  4.76 | ppl   116.98\n",
            "| epoch  29 |   700/ 1019 batches | lr 0.000005 | ms/batch 74.48 | loss  4.77 | ppl   117.92\n",
            "| epoch  29 |   800/ 1019 batches | lr 0.000004 | ms/batch 74.47 | loss  4.76 | ppl   116.81\n",
            "| epoch  29 |   900/ 1019 batches | lr 0.000003 | ms/batch 74.51 | loss  4.76 | ppl   116.59\n",
            "| epoch  29 |  1000/ 1019 batches | lr 0.000003 | ms/batch 74.48 | loss  4.77 | ppl   117.74\n",
            "| end of epoch  29 | time: 78.17s | valid loss  5.18 | valid ppl   177.49\n",
            "| epoch  30 |   100/ 1019 batches | lr 0.000002 | ms/batch 75.27 | loss  4.81 | ppl   122.83\n",
            "| epoch  30 |   200/ 1019 batches | lr 0.000002 | ms/batch 74.47 | loss  4.76 | ppl   116.59\n",
            "| epoch  30 |   300/ 1019 batches | lr 0.000001 | ms/batch 74.51 | loss  4.77 | ppl   117.94\n",
            "| epoch  30 |   400/ 1019 batches | lr 0.000001 | ms/batch 74.43 | loss  4.77 | ppl   117.77\n",
            "| epoch  30 |   500/ 1019 batches | lr 0.000001 | ms/batch 74.38 | loss  4.77 | ppl   117.62\n",
            "| epoch  30 |   600/ 1019 batches | lr 0.000000 | ms/batch 74.49 | loss  4.76 | ppl   117.18\n",
            "| epoch  30 |   700/ 1019 batches | lr 0.000000 | ms/batch 74.47 | loss  4.76 | ppl   116.56\n",
            "| epoch  30 |   800/ 1019 batches | lr 0.000000 | ms/batch 74.37 | loss  4.75 | ppl   116.00\n",
            "| epoch  30 |   900/ 1019 batches | lr 0.000000 | ms/batch 74.45 | loss  4.75 | ppl   116.14\n",
            "| epoch  30 |  1000/ 1019 batches | lr 0.000000 | ms/batch 74.38 | loss  4.77 | ppl   117.95\n",
            "| end of epoch  30 | time: 78.17s | valid loss  5.18 | valid ppl   177.52\n",
            "| End of training | valid loss  5.11 | test ppl   166.33\n"
          ]
        }
      ]
    }
  ]
}