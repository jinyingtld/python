{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MMDetection-Faster-R-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7vyGpn6hM6ukHhj6RpsF0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinyingtld/python/blob/main/MMDetection_Faster_R_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [MMDetection Faster R-CNN 源码详解（一)](https://zhuanlan.zhihu.com/p/166248079)\n",
        "\n",
        "本系列文章会详细剖析 MMDetection 是如何实现 Faster R-CNN 的，在本篇文章（一）中会详细的讲解 Faster R-CNN 中 backbone 相关的代码。\n",
        "\n",
        "下图是 MMDetection 实现的 Faster R-CNN 的结果。R-50 代表的是 ResNet 50，X-101 代表的是 ResNeXt 101。所以在本篇文章中会详细的讲解这两个 backbone 的网络结构和在 MMDetection 中的源码实现。\n",
        "\n",
        "![Image](https://pic3.zhimg.com/80/v2-a5c9e0fe5d3f2ff43e82a570eb813326_720w.jpg)\n",
        "\n",
        "\n",
        "## 一、ResNet\n",
        "\n",
        "![](https://pic4.zhimg.com/80/v2-1155f81ae526e777e575900f3e84b65b_720w.jpg)\n",
        "\n",
        "上图是 ResNet 的网络结构，我们来详细的分析一下。\n",
        "\n",
        "网络的输入是 224×224 的图片，首先会经过 stem 模块。stem 模块对于不同深度的 ResNet 使用的都是相同的结构，都会经过一个conv 7×7（channels：64，stride：2，padding：3） 的卷积。输出特征图的大小为 112×112。然后再次下采样，经过 Max Pooling 3×3（stride：2，padding：1） 得到 56 ×56 的特征图。当经过了 stem 模块，会完成 4 倍的下采样。(x: 224X224X3 -->conv:( 7x7 stride=2,padding=3 output=112x112)-->BN--->ReLu-->Maxpool(3x3, stride=2 padding=1 downsampling out=112/2=56)\n",
        "\n",
        "接下来会进入 4 组堆叠的残差模块，除了经过第一组残差模块特征图大小以及通道数不发生变化以外，后面三组残差模块，每经过一组，特征图的大小都会缩小一半，通道数扩张为原来 2 倍。经过四组残差模块进行特征提取后，特征图的大小变为 7×7，然后对这 7×7 的特征图做全局均值池化(avg Pooling)。最后接上全连接层，将通道数变为类别个数进行输出。\n",
        "\n",
        "在 ResNet 中最主要的结构就是残差模块，我们下面就详细解释一下残差模块的结构。\n",
        "\n",
        "残差模块含有shortcut连接, 会将经过卷积操作的结果和未经过卷积操作的结果相加. (注意: 注意：这里是相加不是拼接）对于相加后的结果再用激活函数计算. 不同的深度的 ResNet，使用了不同的残差模块。深度小于50的ResNet, 使用BasicBlock, 深度大于等于50的ResNet使用Bottleneck. 下面我们就来分别看一下这两种结构。\n",
        "\n",
        "##（一）BasicBlock（深度小于 50 的 ResNet 使用）\n",
        "BasicBlock 用于深度小于 50 的 ResNet（ResNet 18、34）。它的结构如下图：\n",
        "![](https://pic4.zhimg.com/80/v2-8ef93910b9b1d74f45cef442424b31fb_720w.jpg)\n",
        "图三：BasicBlock 结构\n",
        "\n",
        "对于每个BasicBlock, 有两个分支. 一个分支用来正常的卷积,另一个分支用于跳跃连接. 输入一张特征图后会经过两个3x3的卷积提取特征,然后将卷积操作后的特征图与输入的特征图相加，再用 ReLU 激活函数进行计算。在 BasicBlock 中，卷积的通道数不发生改变，可以直接将输入的特征图和卷积后的特征图相加。\n",
        "\n",
        "上面的操作既不会改变特征图的大小，又不会改变特征图的通道数，那么我们如何进行下采样呢？换句话说，也就是上面的情况是针对同一个 stage 之内的 BasicBlock，对于不同 stage 之间的 BasicBlock 我们怎么处理呢（如：conv2_2 和 conv3_1）？\n",
        "\n",
        "我们对每个stage中的第一个block进行更改.将第一个3x3的卷积的步长变为原来的2倍, 通过数也扩大为原来的2倍,padding不变还为1. 这样的化经过卷积后的特征图的大小变为原来的1/2,通道数会扩张为原来的2倍. 对于shortcut分支,使用步长为2的 1×1 卷积用于下采样和扩张通道数，这样特征图的大小变成原来的 1/2，通道数也会扩张为原来的 2 倍。和卷积分支的输出特征图形状相同。然后我们将两个分支的结果相加，再通过 ReLU 激活函数即可。如下图：\n",
        "\n",
        "![](https://pic2.zhimg.com/v2-e849a9d912716e1c213fb5db3bb76a45_r.jpg)图四：stage 和 stage 之间的 BasicBlock\n",
        "\n",
        "当然因为 stem 和第一个 stage 之间的通道数都是 64，且第一个 stage 不需要进行下采样。所以，我们使用图三的 block 即可。]\n",
        "\n",
        "我们来看一下源码：\n"
      ],
      "metadata": {
        "id": "OJDTllqasZ8h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxJdU2-_sSmD"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.utils.checkpoint as cp\n",
        "from mmcv.cnn import (build_conv_layer, build_norm_layer, build_plugin_layer, constant_init, kaiming_init)\n",
        "from mmcv.runner import load_checkpoint\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "\n",
        "from mmdet.utils import get_root_logger\n",
        "from ..builder import BACKBONES\n",
        "from ..utils import ResLayer\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"ResNet 18, 34 使用的block\"\"\"\n",
        "    # 输出通道为输入通道的倍数.(输出通道数 == 输入通道数)\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self,\n",
        "                 inplanes,\n",
        "                 planes,\n",
        "                 stride=1,\n",
        "                 dilation=1,\n",
        "                 downsample=None\n",
        "                 style='pytorch',\n",
        "                 with_cp=False,\n",
        "                 conf_cfg=None,\n",
        "                 norm_cfg=dict(type='BN'),\n",
        "                 dcn=None,\n",
        "                 plugins=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        assert dcn is None, 'Not implemented yet.'\n",
        "        assert plugins is None, 'Not implemented yet.'\n",
        "        # conv3x3 ---> bn1 --> relu --> conv3x3 ---> bn2 --> relu\n",
        "\n",
        "        #bn1, bn2\n",
        "        self.norm1_name, norm1 = build_norm_layer(norm_cfg, planes, postfix=1)\n",
        "        self.norm2_name, norm2 = build_norm_layer(norm_cfg, planes, postfix=2)\n",
        "\n",
        "        # 当conv1: 3x3 conv \n",
        "        # 当conv 为3x3 且padding = dilation时, 原特征图大小只和 stride 有关\n",
        "        self.conv1 = build_conv_layer(\n",
        "            conv_cfg,\n",
        "            inplanes,\n",
        "            planes,\n",
        "            3,\n",
        "            stride=stride,\n",
        "            padding=dilation,\n",
        "            dilation=dilation,\n",
        "            bias=False)\n",
        "        self.add_module(self.norm1_name,norm1)\n",
        "\n",
        "        # conv2: 3x3 conv, stride = 1, padding = 1.\n",
        "        self.conv2 = build_conv_layer(\n",
        "            conv_cfg, planes, planes, 3, padding=1, bias=False)\n",
        "        self.add_module(self.norm2_name, norm2)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "        self.with_cp = with_cp\n",
        "\n",
        "    @property\n",
        "    def norm1(self):\n",
        "        \"\"\"nn.Module: normalization layer after the first convolution layer\"\"\"\n",
        "        return getattr(self, self.norm1_name)\n",
        "    \n",
        "    @property\n",
        "    def norm2(self):\n",
        "        \"\"\"nn.Module: normalization layer after the second convolution layer\"\"\"\n",
        "        return getattr(self, self.norm2_name)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward function.\"\"\"\n",
        "        def _inner_forward(x):\n",
        "            identity = x\n",
        "\n",
        "            out = self.conv1(x)\n",
        "            out = self.norm1(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "            out = self.conv2(out)\n",
        "            out = self.norm2(out)\n",
        "\n",
        "            # 需要保证identity 和 x的宽度相同.\n",
        "            if self.downsample is not None:\n",
        "                identity = self.downsample(x)\n",
        "            \n",
        "            # h(x) = f(x) + x\n",
        "            out += identity\n",
        "\n",
        "            return out\n",
        "\n",
        "        # 加载预训练模型并且需要求导的时候, 使用checkpoint用时间换取空间.\n",
        "        # 这是因为加载权重后可以训练的epoch数少, 可以考虑时间换取空间.\n",
        "        if self.with_cp and x.requires_grad:\n",
        "            # 使用 checkpoint 不保存中间计算的激活值, 在反向传播中重新计算一次中间激活值。\n",
        "            # 即重新运行一次检查点部分的前向传播，这是一种以时间换空间（显存）的方法。\n",
        "            out = cp.checkpoint(_inner_forward, x)\n",
        "        # 不加载权重，从零开始训练。不使用 ckpt，因为训练慢。\n",
        "        else:\n",
        "            out = _inner_forward(x)\n",
        "\n",
        "         # 注意：f(x) + x 之后再进行 relu\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## （二）Bottleneck（深度大于等于 50 的 ResNet 使用）\n",
        "\n",
        "Bottleneck 用于深度大于等于50de ResNet (ResNet 50, 101, 152). 它的结构如下图：\n",
        "\n",
        "![](https://pic2.zhimg.com/80/v2-b37d119bf676d64d30e0f36a72824999_720w.jpg)图五：Bottleneck 结构\n",
        "\n",
        "对于 Bottleneck 来说，它会先使用 1×1 的卷积对输入的通道数进行压缩，然后再使用 3×3 的卷积进行卷积，最后使用 1×1 的卷积恢复通道数。然后将输入与卷积的结果相加并通过激活函数进行计算。\n",
        "\n",
        "对于上面的操作，特征图的大小和输入输出的通道数不发生改变。如果需要下采样。需要将每个 stage 的第一个 Bottleneck 中的 3×3 卷积的步长和通道数设置为原来的 2 倍。对于 shortcut 分支，我们使用 1×1 步长为 2，通道数为原来 2 倍的卷积核进行下采样。如下图：\n",
        "\n",
        "![](https://pic1.zhimg.com/80/v2-61b2c58524945a7af86b33b95c6f35ac_720w.jpg)\n",
        "图六：stage 和 stage 之间的 BasicBlock\n",
        "\n",
        "stem 和第一个 stage 之前。只需要扩大通道数不需要下采样，所以，我们只用将上图（图六）的 Bottleneck 中的 3×3 的卷积的步长设置为 1 即可。\n",
        "\n",
        "我们来看一下源码：\n"
      ],
      "metadata": {
        "id": "l5f5VjtiInpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    \"\"\"ResNet 50, 101, 152 使用的 block\n",
        "\n",
        "    Args:\n",
        "        style:(str) 'pytorch 或 'caffe'.\n",
        "                    如果使用 'pytorch', block 中 stride 为 2 的卷积层是 3x3 conv, stride=2\n",
        "                    如果使用 'caffe',   block 中 stride 为 2 的卷积层是 1x1 conv, stride=2\n",
        "    \"\"\"\n",
        "    # 输出通道数为输入通道数的倍数. (输出通道数 == 4 × 输入通道数)\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self,\n",
        "                 inplanes,\n",
        "                 planes,\n",
        "                 stride=1,\n",
        "                 dilation=1,\n",
        "                 downsample=None,\n",
        "                 style='pytorch',\n",
        "                 with_cp=False,\n",
        "                 conv_cfg=None,\n",
        "                 norm_cfg=dict(type='BN'),\n",
        "                 dcn=None,\n",
        "                 plugins=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        assert style in ['pytorch', 'caffe']\n",
        "        assert dcn is None or isinstance(dcn, dict)\n",
        "        assert plugins is None or isinstance(plugins, list)\n",
        "        if plugins is not None:\n",
        "            allowed_position = ['after_conv1', 'after_conv2', 'after_conv3']\n",
        "            assert all(p['position'] in allowed_position for p in plugins)\n",
        "\n",
        "        self.inplanes = inplanes\n",
        "        self.planes = planes\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "        self.style = style\n",
        "        self.with_cp = with_cp\n",
        "        self.conv_cfg = conv_cfg\n",
        "        self.norm_cfg = norm_cfg\n",
        "        self.dcn = dcn\n",
        "        self.with_dcn = dcn is not None\n",
        "        self.plugins = plugins\n",
        "        self.with_plugins = plugins is not None\n",
        "\n",
        "        if self.with_plugins:\n",
        "            # collect plugins for conv1/conv2/conv3\n",
        "            self.after_conv1_plugins = [\n",
        "                plugin['cfg'] for plugin in plugins\n",
        "                if plugin['position'] == 'after_conv1'\n",
        "            ]\n",
        "            self.after_conv2_plugins = [\n",
        "                plugin['cfg'] for plugin in plugins\n",
        "                if plugin['position'] == 'after_conv2'\n",
        "            ]\n",
        "            self.after_conv3_plugins = [\n",
        "                plugin['cfg'] for plugin in plugins\n",
        "                if plugin['position'] == 'after_conv3'\n",
        "            ]\n",
        "\n",
        "        if self.style == 'pytorch':\n",
        "            self.conv1_stride = 1\n",
        "            self.conv2_stride = stride\n",
        "        else:\n",
        "            self.conv1_stride = stride\n",
        "            self.conv2_stride = 1\n",
        "\n",
        "        # conv1x1 --> bn1 --> relu\n",
        "        # conv3x3 --> bn2 --> relu\n",
        "        # conv1x1 --> bn3\n",
        "        self.norm1_name, norm1 = build_norm_layer(norm_cfg, planes, postfix=1)\n",
        "        self.norm2_name, norm2 = build_norm_layer(norm_cfg, planes, postfix=2)\n",
        "        self.norm3_name, norm3 = build_norm_layer(\n",
        "            norm_cfg, planes * self.expansion, postfix=3\n",
        "        )\n",
        "\n",
        "        self.conv1 = build_conv_layer(\n",
        "            conv_cfg,\n",
        "            inplanes,\n",
        "            planes,\n",
        "            kernel_size=1,\n",
        "            stride=self.conv1_stride,\n",
        "            bias=False)\n",
        "        self.add_module(self.norm1_name, norm1)\n",
        "        fallback_on_stride = False\n",
        "        if self.with_dcn:\n",
        "            fallback_on_stride = dcn.pop('fallback_on_stride', False)\n",
        "        if not self.with_dcn or fallback_on_stride:\n",
        "            self.conv2 = build_conv_layer(\n",
        "                conv_cfg,\n",
        "                planes,\n",
        "                planes,\n",
        "                kernel_size=3,\n",
        "                stride=self.conv2_stride,\n",
        "                padding=dilation,\n",
        "                dilation=dilation,\n",
        "                bias=False)\n",
        "        else:\n",
        "            assert self.conv_cfg is None, 'conv_cfg must be None for DCN'\n",
        "            self.conv2 = build_conv_layer(\n",
        "                dcn,\n",
        "                planes,\n",
        "                planes,\n",
        "                kernel_size=3,\n",
        "                stride=self.conv2_stride,\n",
        "                padding=dilation,\n",
        "                dilation=dilation,\n",
        "                bias=False)\n",
        "\n",
        "        self.add_module(self.norm2_name, norm2)\n",
        "        self.conv3 = build_conv_layer(\n",
        "            conv_cfg,\n",
        "            planes,\n",
        "            planes * self.expansion,\n",
        "            kernel_size=1,\n",
        "            bias=False)\n",
        "        self.add_module(self.norm3_name, norm3)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample \n",
        "\n",
        "        if self.with_plugins:\n",
        "            self.after_conv1_plugin_names = self.make_block_plugins(\n",
        "                planes, self.after_conv1_plugins)\n",
        "            self.after_conv2_plugin_names = self.make_block_plugins(\n",
        "                planes, self.after_conv2_plugins)\n",
        "            self.after_conv3_plugin_names = self.make_block_plugins(\n",
        "                planes * self.expansion, self.after_conv3_plugins)\n",
        "\n",
        "    def make_block_plugins(self, in_channels, plugins):\n",
        "        \"\"\"make plugins for block.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): Input channels of plugin.\n",
        "            plugins (list[dict]): List of plugins cfg to build.\n",
        "\n",
        "        Returns:\n",
        "            list[str]: List of the names of plugin.\n",
        "        \"\"\"\n",
        "        assert isinstance(plugins, list)\n",
        "        plugin_names = []\n",
        "        for plugin in plugins:\n",
        "            plugin = plugin.copy()\n",
        "            name, layer = build_plugin_layer(\n",
        "                plugin,\n",
        "                in_channels=in_channels,\n",
        "                postfix=plugin.pop('postfix', ''))\n",
        "            assert not hasattr(self, name), f'duplicate plugin {name}'\n",
        "            self.add_module(name, layer)\n",
        "            plugin_names.append(name)\n",
        "        return plugin_names\n",
        "\n",
        "    def forward_plugin(self, x, plugin_names):\n",
        "        out = x\n",
        "        for name in plugin_names:\n",
        "            out = getattr(self, name)(x)\n",
        "        return out\n",
        "\n",
        "    @property\n",
        "    def norm1(self):\n",
        "        \"\"\"nn.Module: normalization layer after the first convolution layer\"\"\"\n",
        "        return getattr(self, self.norm1_name)\n",
        "\n",
        "    @property\n",
        "    def norm2(self):\n",
        "        \"\"\"nn.Module: normalization layer after the second convolution layer\"\"\"\n",
        "        return getattr(self, self.norm2_name)\n",
        "\n",
        "    @property\n",
        "    def norm3(self):\n",
        "        \"\"\"nn.Module: normalization layer after the third convolution layer\"\"\"\n",
        "        return getattr(self, self.norm3_name)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward function.\"\"\"\n",
        "\n",
        "        def _inner_forward(x):\n",
        "            identity = x\n",
        "\n",
        "            out = self.conv1(x)\n",
        "            out = self.norm1(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "            if self.with_plugins:\n",
        "                out = self.forward_plugin(out, self.after_conv1_plugin_names)\n",
        "\n",
        "            out = self.conv2(out)\n",
        "            out = self.norm2(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "            if self.with_plugins:\n",
        "                out = self.forward_plugin(out, self.after_conv2_plugin_names)\n",
        "\n",
        "            out = self.conv3(out)\n",
        "            out = self.norm3(out)\n",
        "\n",
        "            if self.with_plugins:\n",
        "                out = self.forward_plugin(out, self.after_conv3_plugin_names)\n",
        "\n",
        "            if self.downsample is not None:\n",
        "                identity = self.downsample(x)\n",
        "\n",
        "            out += identity\n",
        "\n",
        "            return out\n",
        "\n",
        "        if self.with_cp and x.requires_grad:\n",
        "            out = cp.checkpoint(_inner_forward, x)\n",
        "        else:\n",
        "            out = _inner_forward(x)\n",
        "\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "1pDgpc_6Mg41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 下面我们来看一下 ResNet 类的源码："
      ],
      "metadata": {
        "id": "k6YCR-UPTGv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Moduel):\n",
        "    \"\"\"ResNet backbone.\n",
        "\n",
        "    Args:\n",
        "        depth:                     (int)   ResNet 的深度, 可以是 {18, 34, 50, 101, 152}.\n",
        "        in_channels:               (int)   输入图像的通道数(默认: 3).\n",
        "        stem_channels:             (int)   stem 的通道数(默认: 64).\n",
        "        base_channels:             (int)   ResNet 的 res layer 的基础通道数(默认: 64).\n",
        "        num_stages:                (int)   使用 ResNet 的 stage 数量(默认: 4).\n",
        "        strides:         (Sequence[int])   每个 stage 的第一个 block 的 stride, 如果为 2 进行 2 倍下采样.\n",
        "        dilations:       (Sequence[int])   每个 stage 中所有 block 的第一个卷积层的 dilation.\n",
        "        out_indices:     (Sequence[int])   需要输出的 stage 的索引.\n",
        "        style:                     (str)   'pytorch' 或 'caffe'.\n",
        "                                           如果使用 'pytorch', block 中 stride 为 2 的卷积层是 3x3 conv2, stride=2\n",
        "                                           如果使用 'caffe',   block 中 stride 为 2 的卷积层是 1x1 conv1, stride=2\n",
        "        deep_stem:                (bool)   如果为 True, 将 stem 的 7x7 conv 替换为 3 个 3x3 conv.\n",
        "        avg_down:                 (bool)   在下采样的时候使用 Avg pool 2x2 stride=2 代替带步长的卷积.\n",
        "        frozen_stages:             (int)   冻结的 stage 数(停止更新梯度, 并开启eval模式), -1 代表不冻结.\n",
        "        conv_cfg:                 (dict)   构建 conv 的 config.\n",
        "        norm_cfg:                 (dict)   构建 norm 的 config.\n",
        "        norm_eval:                (bool)   是否设置 norm 层为 eval 模式. 即冻结参数状态(mean, var).\n",
        "        dcn:                      (dict)   构建 DCN 的 config.\n",
        "        stage_with_dcn: (Sequence[bool])   需要使用 DCN 的 stage.\n",
        "        plugins:            (list[dict])   为 stage 提供插件.\n",
        "        with_cp:                  (bool)   是否加载 checkpoint. 使用 checkpoint 会节省一部分内存, 同时会减少训练时间.\n",
        "        zero_init_residual:       (bool)   是否使用 0 对所有 block 中的最后一个 norm 层初始化, 使其为恒等映射.\n",
        "\n",
        "    Example:\n",
        "        >>> from mmdet.models import ResNet\n",
        "        >>> import torch\n",
        "        >>> self = ResNet(depth=18)\n",
        "        >>> self.eval()\n",
        "        >>> inputs = torch.rand(1, 3, 32, 32)\n",
        "        >>> level_outputs = self.forward(inputs)\n",
        "        >>> for level_out in level_outputs:\n",
        "        ...     print(tuple(level_out.shape))\n",
        "        (1, 64, 8, 8)\n",
        "        (1, 128, 4, 4)\n",
        "        (1, 256, 2, 2)\n",
        "        (1, 512, 1, 1)\n",
        "    \"\"\"\n",
        "    arch_settings = {\n",
        "        18: (BasicBlock, (2, 2, 2, 2)),\n",
        "        34: (BasicBlock, (3, 4, 6, 3)),\n",
        "        50: (Bottleneck, (3, 4, 6, 3)),\n",
        "        101: (Bottleneck, (3, 4, 23, 3)),\n",
        "        152: (Bottleneck, (3, 8, 36, 3))\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 depth,\n",
        "                 in_channels=3,\n",
        "                 stem_channels=64,\n",
        "                 base_channels=64,\n",
        "                 num_stages=4,\n",
        "                 strides=(1, 2, 2, 2),\n",
        "                 dilations=(1, 1, 1, 1),\n",
        "                 out_indices=(0, 1, 2, 3),\n",
        "                 style='pytorch',\n",
        "                 deep_stem=False,\n",
        "                 avg_down=False,\n",
        "                 frozen_stages=-1,\n",
        "                 conv_cfg=None,\n",
        "                 norm_cfg=dict(type='BN', requires_grad=True),\n",
        "                 norm_eval=True,\n",
        "                 dcn=None,\n",
        "                 stage_with_dcn=(False, False, False, False),\n",
        "                 plugins=None,\n",
        "                 with_cp=False,\n",
        "                 zero_init_residual=True):\n",
        "        super(ResNet, self).__init__()\n",
        "        # ========================== 初始化属性 =============================\n",
        "        if depth not in self.arch_settings:\n",
        "            raise KeyError(f'invalid depth {depth} for resnet')\n",
        "        self.depth = depth\n",
        "        self.stem_channels = stem_channels\n",
        "        self.base_channels = base_channels\n",
        "        self.num_stages = num_stages\n",
        "        assert num_stages >= 1 and num_stages <= 4\n",
        "        self.strides = strides\n",
        "        self.dilations = dilations\n",
        "        assert len(strides) == len(dilations) == num_stages\n",
        "        self.out_indices = out_indices\n",
        "        assert max(out_indices) < num_stages\n",
        "        self.style = style\n",
        "        self.deep_stem = deep_stem\n",
        "        self.avg_down = avg_down\n",
        "        self.frozen_stages = frozen_stages\n",
        "        self.conv_cfg = conv_cfg\n",
        "        self.norm_cfg = norm_cfg\n",
        "        self.with_cp = with_cp\n",
        "        self.norm_eval = norm_eval\n",
        "        self.dcn = dcn\n",
        "        self.stage_with_dcn = stage_with_dcn\n",
        "        if dcn is not None:\n",
        "            assert len(stage_with_dcn) == num_stages\n",
        "        self.plugins = plugins\n",
        "        self.zero_init_residual = zero_init_residual\n",
        "        # ===================================================================\n",
        "        self.block, stage_blocks = self.arch_settings[depth]\n",
        "        self.stage_blocks = stage_blocks[:num_stages]\n",
        "\n",
        "        # stem 层\n",
        "        self.inplanes = stem_channels\n",
        "        self._make_stem_layer(in_channels, stem_channels)\n",
        "\n",
        "        # res 层\n",
        "        self.res_layers = []\n",
        "        for i, num_blocks in enumerate(self.stage_blocks):\n",
        "            stride = strides[i]\n",
        "            dilation = dilations[i]\n",
        "            dcn = self.dcn if self.stage_with_dcn[i] else None\n",
        "            if plugins is not None:\n",
        "                stage_plugins = self.make_stage_plugins(plugins, i)\n",
        "            else:\n",
        "                stage_plugins = None\n",
        "            planes = base_channels * 2**i\n",
        "            res_layer = self.make_res_layer(\n",
        "                block=self.block,\n",
        "                inplanes=self.inplanes,\n",
        "                planes=planes,\n",
        "                num_blocks=num_blocks,\n",
        "                stride=stride,\n",
        "                dilation=dilation,\n",
        "                style=self.style,\n",
        "                avg_down=self.avg_down,\n",
        "                with_cp=with_cp,\n",
        "                conv_cfg=conv_cfg,\n",
        "                norm_cfg=norm_cfg,\n",
        "                dcn=dcn,\n",
        "                plugins=stage_plugins)\n",
        "            self.inplanes = planes * self.block.expansion\n",
        "            layer_name = f'layer{i + 1}'\n",
        "            self.add_module(layer_name, res_layer)\n",
        "            self.res_layers.append(layer_name)\n",
        "\n",
        "        self._freeze_stages()\n",
        "\n",
        "        self.feat_dim = self.block.expansion * base_channels * 2**(\n",
        "            len(self.stage_blocks) - 1)\n",
        "\n",
        "    def make_stage_plugins(self, plugins, stage_idx):\n",
        "        \"\"\"Make plugins for ResNet ``stage_idx`` th stage.\n",
        "\n",
        "        Currently we support to insert ``context_block``,\n",
        "        ``empirical_attention_block``, ``nonlocal_block`` into the backbone\n",
        "        like ResNet/ResNeXt. They could be inserted after conv1/conv2/conv3 of\n",
        "        Bottleneck.\n",
        "\n",
        "        An example of plugins format could be:\n",
        "\n",
        "        Examples:\n",
        "            >>> plugins=[\n",
        "            ...     dict(cfg=dict(type='xxx', arg1='xxx'),\n",
        "            ...          stages=(False, True, True, True),\n",
        "            ...          position='after_conv2'),\n",
        "            ...     dict(cfg=dict(type='yyy'),\n",
        "            ...          stages=(True, True, True, True),\n",
        "            ...          position='after_conv3'),\n",
        "            ...     dict(cfg=dict(type='zzz', postfix='1'),\n",
        "            ...          stages=(True, True, True, True),\n",
        "            ...          position='after_conv3'),\n",
        "            ...     dict(cfg=dict(type='zzz', postfix='2'),\n",
        "            ...          stages=(True, True, True, True),\n",
        "            ...          position='after_conv3')\n",
        "            ... ]\n",
        "            >>> self = ResNet(depth=18)\n",
        "            >>> stage_plugins = self.make_stage_plugins(plugins, 0)\n",
        "            >>> assert len(stage_plugins) == 3\n",
        "\n",
        "        Suppose ``stage_idx=0``, the structure of blocks in the stage would be:\n",
        "\n",
        "        .. code-block:: none\n",
        "\n",
        "            conv1-> conv2->conv3->yyy->zzz1->zzz2\n",
        "\n",
        "        Suppose 'stage_idx=1', the structure of blocks in the stage would be:\n",
        "\n",
        "        .. code-block:: none\n",
        "\n",
        "            conv1-> conv2->xxx->conv3->yyy->zzz1->zzz2\n",
        "\n",
        "        If stages is missing, the plugin would be applied to all stages.\n",
        "\n",
        "        Args:\n",
        "            plugins (list[dict]): List of plugins cfg to build. The postfix is\n",
        "                required if multiple same type plugins are inserted.\n",
        "            stage_idx (int): Index of stage to build\n",
        "\n",
        "        Returns:\n",
        "            list[dict]: Plugins for current stage\n",
        "        \"\"\"\n",
        "        stage_plugins = []\n",
        "        for plugin in plugins:\n",
        "            plugin = plugin.copy()\n",
        "            stages = plugin.pop('stages', None)\n",
        "            assert stages is None or len(stages) == self.num_stages\n",
        "            # whether to insert plugin into current stage\n",
        "            if stages is None or stages[stage_idx]:\n",
        "                stage_plugins.append(plugin)\n",
        "\n",
        "        return stage_plugins\n",
        "\n",
        "    def make_res_layer(self, **kwargs):\n",
        "        \"\"\"Pack all blocks in a stage into a ``ResLayer``.\"\"\"\n",
        "        return ResLayer(**kwargs)\n",
        "\n",
        "    @property\n",
        "    def norm1(self):\n",
        "        \"\"\"nn.Module: the normalization layer named \"norm1\" \"\"\"\n",
        "        return getattr(self, self.norm1_name)\n",
        "\n",
        "     def _make_stem_layer(self, in_channels, stem_channels):\n",
        "        # 使用 deep stem, 即 3 个 3x3 conv.\n",
        "        if self.deep_stem:\n",
        "            self.stem = nn.Sequential(\n",
        "                build_conv_layer(\n",
        "                    self.conv_cfg,\n",
        "                    in_channels,\n",
        "                    stem_channels // 2,\n",
        "                    kernel_size=3,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    bias=False),\n",
        "                build_norm_layer(self.norm_cfg, stem_channels // 2)[1],\n",
        "                nn.ReLU(inplace=True),\n",
        "                build_conv_layer(\n",
        "                    self.conv_cfg,\n",
        "                    stem_channels // 2,\n",
        "                    stem_channels // 2,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=1,\n",
        "                    bias=False),\n",
        "                build_norm_layer(self.norm_cfg, stem_channels // 2)[1],\n",
        "                nn.ReLU(inplace=True),\n",
        "                build_conv_layer(\n",
        "                    self.conv_cfg,\n",
        "                    stem_channels // 2,\n",
        "                    stem_channels,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=1,\n",
        "                    bias=False),\n",
        "                build_norm_layer(self.norm_cfg, stem_channels)[1],\n",
        "                nn.ReLU(inplace=True))\n",
        "        # 使用原版的 stem 即 7x7 conv\n",
        "        else:\n",
        "            self.conv1 = build_conv_layer(\n",
        "                self.conv_cfg,\n",
        "                in_channels,\n",
        "                stem_channels,\n",
        "                kernel_size=7,\n",
        "                stride=2,\n",
        "                padding=3,\n",
        "                bias=False)\n",
        "            self.norm1_name, norm1 = build_norm_layer(\n",
        "                self.norm_cfg, stem_channels, postfix=1)\n",
        "            self.add_module(self.norm1_name, norm1)\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def _freeze_stages(self):\n",
        "        # 冻结 stem 层 --> stage 为 0\n",
        "        if self.frozen_stages >= 0:\n",
        "            if self.deep_stem:\n",
        "                self.stem.eval()\n",
        "                for param in self.stem.parameters():\n",
        "                    param.requires_grad = False\n",
        "            else:\n",
        "                self.norm1.eval()\n",
        "                for m in [self.conv1, self.norm1]:\n",
        "                    for param in m.parameters():\n",
        "                        param.requires_grad = False\n",
        "\n",
        "        # 冻结 res 层  --> stage 大于 0\n",
        "        for i in range(1, self.frozen_stages + 1):\n",
        "            m = getattr(self, f'layer{i}')\n",
        "            m.eval()\n",
        "            for param in m.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def init_weights(self, pretrained=None):\n",
        "        \"\"\"Initialize the weights in backbone.\n",
        "\n",
        "        Args:\n",
        "            pretrained (str, optional): Path to pre-trained weights.\n",
        "                Defaults to None.\n",
        "        \"\"\"\n",
        "        if isinstance(pretrained, str):\n",
        "            logger = get_root_logger()\n",
        "            load_checkpoint(self, pretrained, strict=False, logger=logger)\n",
        "        elif pretrained is None:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Conv2d):\n",
        "                    kaiming_init(m)\n",
        "                elif isinstance(m, (_BatchNorm, nn.GroupNorm)):\n",
        "                    constant_init(m, 1)\n",
        "\n",
        "            if self.dcn is not None:\n",
        "                for m in self.modules():\n",
        "                    if isinstance(m, Bottleneck) and hasattr(\n",
        "                            m.conv2, 'conv_offset'):\n",
        "                        constant_init(m.conv2.conv_offset, 0)\n",
        "\n",
        "            if self.zero_init_residual:\n",
        "                for m in self.modules():\n",
        "                    if isinstance(m, Bottleneck):\n",
        "                        constant_init(m.norm3, 0)\n",
        "                    elif isinstance(m, BasicBlock):\n",
        "                        constant_init(m.norm2, 0)\n",
        "        else:\n",
        "            raise TypeError('pretrained must be a str or None')\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward function.\"\"\"\n",
        "        if self.deep_stem:\n",
        "            x = self.stem(x)\n",
        "        else:\n",
        "            x = self.conv1(x)\n",
        "            x = self.norm1(x)\n",
        "            x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        outs = []\n",
        "        for i, layer_name in enumerate(self.res_layers):\n",
        "            res_layer = getattr(self, layer_name)\n",
        "            x = res_layer(x)\n",
        "            if i in self.out_indices:\n",
        "                outs.append(x)\n",
        "        return tuple(outs)\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        \"\"\"Convert the model into training mode while keep normalization layer\n",
        "        freezed.\"\"\"\n",
        "        super(ResNet, self).train(mode)\n",
        "        self._freeze_stages()\n",
        "        if mode and self.norm_eval:\n",
        "            for m in self.modules():\n",
        "                # trick: eval have effect on BatchNorm only\n",
        "                if isinstance(m, _BatchNorm):\n",
        "                    m.eval()"
      ],
      "metadata": {
        "id": "x43IGMFqTTBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[更多ResNeXt](https://zhuanlan.zhihu.com/p/166248079)"
      ],
      "metadata": {
        "id": "RhI4TgxZU-sR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [MMDetection Faster R-CNN 源码详解（二）](https://zhuanlan.zhihu.com/p/183098688)\n",
        "\n",
        "本篇文章中，会重点剖析 MMDetection 实现的 Faster R-CNN 中的 neck 相关的源码。\n",
        "\n",
        "## 一、FPN 的思想(Feature Pyramid Network)\n",
        "\n",
        "在 MMDetection 中，Faster R-CNN 的 baseline 结合了 FPN，通过 FPN 会大大的提升检测的效果。为什么 FPN 有这么强的作用呢？我们从如下两点分析\n",
        "\n",
        "1. 提取了多尺度的特征: 相对于单尺度的特征，多尺度的特征对大中小物体都能覆盖。对于 CNN 网络，浅层特征的特征途较大, 感受野较小, 方便检测小物体. 深层特征的特征图较小, 感受野较大, 方便检测大物体. 所以使用多尺度的特征对于不同大小的物体都有很好的覆盖效果。\n",
        "\n",
        "2. 融合了各个尺度的特征：对于浅层的特征图，对位置敏感但是语义信息较弱。深层的特征图，对位置不敏感但是语义信息较强。那么我们就可以将当前的尺度与更深层的尺度的特征图相融合。这样既有深层的语义信息又对位置敏感。\n",
        "\n",
        "\n",
        "## 二、Faster R-CNN 结合 FPN\n",
        "接下来我们就来看一下如何将 Faster R-CNN 与 FPN 向结合，下图是将 Faster R-CNN 与 FPN 融合的网络结构。\n",
        "\n",
        "![](https://pic1.zhimg.com/80/v2-4aee99e6842420bf682433ff4c0ff720_720w.jpg) Faster R-CNN 结合 FPN 的网络结构\n",
        "\n",
        "\n",
        "拿到 backbone 的后四个尺度的输出，也就是 C2 ～ C5。以 resnet 50 为例，通道数分别为：256、512、1024、2048。先使用 1×1 的卷积，将 resnet 输出的特征图压缩通道数到 256，得到 P2 ～ P5。然后将浅层的特征图与深层的特征图相加（需要先上采样，保证特征图大小相同再相加）。再使用 3 × 3 的卷积核进行卷积，此步骤会将相加的特征进一步融合，得到 FPN 输出的特征，也就是 P2 ～ P5。在 P5 上使用 1 × 1 步长为 2 的 max pool 进行下采样，得到 P6。所以当数据经过 FPN 后，我们会拿到五个尺度的特征，也就是 P2 ～ P6。每个特征的通道数都是 256。因为每个尺度输出的特征图的通道数固定，所以我们就可以使用相同的头部进行预测了。\n",
        "\n",
        "![](https://pic1.zhimg.com/80/v2-c0172be282021a1029f7b72b51079ffe_1440w.jpg)\n",
        "\n",
        "\n",
        "![](https://pic2.zhimg.com/v2-e49ebcf931b5cf424ed311338f9ff35d_b.jpg)\n",
        "\n",
        "\n",
        "## 三、RetinaNet 中的 FPN\n",
        "\n",
        "![](https://pic1.zhimg.com/80/v2-b5a2faa28bd62f532d4fb405159f15dc_720w.jpg)\n",
        "\n",
        "在 RetinaNet 中，利用的 backbone 的特征是 C3 ～ C5。将 C3 ～ C5 的特征图经过 1×1 的卷积压缩通道数，再将深层特征图与浅层特征图相加。然后使用 3 × 3 的卷积进一步融合特征。这样操作后，我们提取了从 P3 ～ P5 的特征。对于 C5，我们用 3 × 3 步长为 2，padding 为 1 的卷积进行两次下采样就得到 FPN 的 P6 和 P7。\n",
        "\n"
      ],
      "metadata": {
        "id": "G3gGKZ5BVLeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 四、FPN 源码分析\n",
        "（一）Faster R-CNN 的 FPN 配置\n",
        "紧接着，将x输入到neck中,也就是fpn结构中，我这里fpn的设置参数为"
      ],
      "metadata": {
        "id": "0B0o04mTVqIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neck=dict(\n",
        "        type='FPN',\n",
        "        in_channels=[256, 512, 1024, 2048],\n",
        "        out_channels=256,\n",
        "        num_outs=5),"
      ],
      "metadata": {
        "id": "uy6mhzVaa25e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "（二）RetinaNet 的 FPN 配置"
      ],
      "metadata": {
        "id": "-pKD1d2ra8lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neck=dict(\n",
        "        type='FPN',\n",
        "        in_channels=[256, 512, 1024, 2048],\n",
        "        out_channels=256,\n",
        "        start_level=1,\n",
        "        add_extra_convs='on_input',\n",
        "        num_outs=5),"
      ],
      "metadata": {
        "id": "qE56vomna9i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "结合上面的配置文件我们来看一下源码：\n"
      ],
      "metadata": {
        "id": "BExuOoPJbBH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from mmcv.cnn import ConvModule, xavier_init\n",
        "\n",
        "from mmdet.core import auto_fp16\n",
        "from ..builder import NECKS\n",
        "\n",
        "@NECKS.register_module()\n",
        "class FPN(nn.Module):\n",
        "    \"\"\"Feature Pyramid Network.  (https://arxiv.org/abs/1612.03144)\n",
        "\n",
        "    Args:\n",
        "        in_channels:                    (List[int])     每个尺度的输入通道数, 也是 backbone 的输出通道数.\n",
        "        out_channels:                  (int)     fpn 的输出通道数, 所有尺度的输出通道数相同, 都是一个值.\n",
        "        num_outs:                      (int)     输出 stage 的个数.(可以附加额外的层, num_outs 不一定等于 in_channels)\n",
        "        start_level:                   (int)     使用 backbone 的起始 stage 索引, 默认为 0.\n",
        "        end_level:                     (int)     使用 backbone 的终止 stage 索引。\n",
        "                                                默认为 -1, 代表到最后一层(包括)全使用.\n",
        "        add_extra_convs:        (bool | str)     可以是 bool 或 str:\n",
        "                                                (bool)  bool 代表是否添加额外的层.(默认值: False)\n",
        "                                                        True:   在最顶层 feature map 上添加额外的卷积层,\n",
        "                                                                具体的模式需要 extra_convs_on_inputs 指定.\n",
        "                                                        False:  不添加额外的卷积层\n",
        "                                                (str)   str  需要指定 extra convs 的输入的 feature map 的来源\n",
        "                                                        'on_input':     最高层的 feature map 作为 extra 的输入\n",
        "                                                        'on_lateral':   最高层的 lateral 结果 作为 extra 的输入\n",
        "                                                        'on_output':    最高层的经过 conv 的 lateral 结果作为 extra 的输入\n",
        "        extra_convs_on_inputs:  (bool, deprecated)  True  等同于 `add_extra_convs='on_input'\n",
        "                                                    False 等同于 `add_extra_convs='on_output'\n",
        "                                                    默认值为True\n",
        "        relu_before_extra_convs:      (bool)     是否在 extra conv 前使用 relu. (默认值: False)\n",
        "        no_norm_on_lateral:           (bool)     是否对 lateral 使用 bn. (默认值: False)\n",
        "        conv_cfg:                     (dict)     构建 conv 层的 config 字典. (默认值: None)\n",
        "        norm_cfg:                     (dict)     构建  bn  层的 config 字典. (默认值: None)\n",
        "        act_cfg:                      (dict)     构建 activation  层的 config 字典. (默认值: None)\n",
        "        upsample_cfg:                 (dict)     构建 interpolate 层的 config 字典. (默认值: `dict(mode='nearest')`)                                                    \n",
        "        \n",
        "        Example:\n",
        "        >>> import torch\n",
        "        >>> in_channels = [2, 3, 5, 7]\n",
        "        >>> scales = [340, 170, 84, 43]\n",
        "        >>> inputs = [torch.rand(1, c, s, s)\n",
        "        ...           for c, s in zip(in_channels, scales)]\n",
        "        >>> self = FPN(in_channels, 11, len(in_channels)).eval()\n",
        "        >>> outputs = self.forward(inputs)\n",
        "        >>> for i in range(len(outputs)):\n",
        "        ...     print(f'outputs[{i}].shape = {outputs[i].shape}')\n",
        "        outputs[0].shape = torch.Size([1, 11, 340, 340])\n",
        "        outputs[1].shape = torch.Size([1, 11, 170, 170])\n",
        "        outputs[2].shape = torch.Size([1, 11, 84, 84])\n",
        "        outputs[3].shape = torch.Size([1, 11, 43, 43])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 num_outs,\n",
        "                 start_level=0,\n",
        "                 end_level=-1,\n",
        "                 add_extra_convs=False,\n",
        "                 extra_convs_on_inputs=True,\n",
        "                 relu_before_extra_convs=False,\n",
        "                 no_norm_on_lateral=False,\n",
        "                 conv_cfg=None,\n",
        "                 norm_cfg=None,\n",
        "                 act_cfg=None,\n",
        "                 upsample_cfg=dict(mode='nearest')):\n",
        "        super(FPN, self).__init__()\n",
        "        assert isinstance(in_channels, list)\n",
        "        self.in_channels = in_channels          # [256, 512, 1024, 2048]\n",
        "        self.out_channels = out_channels        # 256\n",
        "        self.num_ins = len(in_channels)         # 4\n",
        "        self.num_outs = num_outs                # 5\n",
        "        self.relu_before_extra_convs = relu_before_extra_convs  # False\n",
        "        self.no_norm_on_lateral = no_norm_on_lateral            # False\n",
        "        self.fp16_enabled = False\n",
        "        self.upsample_cfg = upsample_cfg.copy()\n",
        "\n",
        "        # end_level 是对 backbone 输出的尺度中使用的最后一个尺度的索引\n",
        "        # 如果是 -1 表示使用 backbone 最后一个 feature map, 作为最终的索引.\n",
        "        if end_level == -1:\n",
        "            self.backbone_end_level = self.num_ins      #4\n",
        "            # 因为还有 extra conv 所以存在 num_outs > num_ins - start_level 的情况\n",
        "            assert num_outs >= self.num_ins - start_level\n",
        "        else:\n",
        "            # 如果 end_level < inputs, 说明不使用 backbone 全部的尺度, 并且不会提供额外的层.\n",
        "            self.backbone_end_level = end_level\n",
        "            assert end_level <= len(in_channels)\n",
        "            assert num_outs == end_level - start_level\n",
        "\n",
        "        self.start_level = start_level                      # 0\n",
        "        self.end_level = end_level                          # -1\n",
        "        self.add_extra_convs = add_extra_convs              # False\n",
        "        assert isinstance(add_extra_convs, (str, bool))\n",
        "        # add_extra_convs 可以是 bool 或 str\n",
        "        # 1. add_extra_convs 是 str\n",
        "        if isinstance(add_extra_convs, str):\n",
        "            # 确保 add_extra_convs 是 'on_input', 'on_lateral' 或 'on_output'\n",
        "            assert add_extra_convs in ('on_input', 'on_lateral', 'on_output')\n",
        "        # 2. add_extra_convs 是 bool, 需要看 extra_convs_on_inputs\n",
        "        elif add_extra_convs:\n",
        "            if extra_convs_on_inputs:\n",
        "                # For compatibility with previous release\n",
        "                # TODO: deprecate `extra_convs_on_inputs`\n",
        "                self.add_extra_convs = 'on_input'\n",
        "            else:\n",
        "                self.add_extra_convs = 'on_output'\n",
        "\n",
        "        self.lateral_convs = nn.ModuleList()\n",
        "        self.fpn_convs = nn.ModuleList()\n",
        "\n",
        "        # 构建Lateral conv 和 fpn conv \n",
        "        for i in range(self.start_level, self.backbone_end_level):\n",
        "            # 水平卷积(lateral conv): 1×1, C=256,\n",
        "            l_conv = ConvModule(\n",
        "                in_channels[i],\n",
        "                out_channels,\n",
        "                1,\n",
        "                conv_cfg=conv_cfg,\n",
        "                norm_cfg=norm_cfg if not self.no_norm_on_lateral else None,\n",
        "                act_cfg=act_cfg,\n",
        "                inplace=False)\n",
        "            # fpn 输出卷积: 3×3, C=256, P=1\n",
        "            fpn_conv = ConvModule(\n",
        "                out_channels,\n",
        "                out_channels,\n",
        "                3,\n",
        "                padding=1,\n",
        "                conv_cfg=conv_cfg,\n",
        "                norm_cfg=norm_cfg,\n",
        "                act_cfg=act_cfg,\n",
        "                inplace=False)\n",
        "            \n",
        "            self.lateral_convs.append(l_conv)\n",
        "            self.fpn_convs.append(fpn_conv)\n",
        "\n",
        "        # add extra conv layers (e.g., RetinaNet)\n",
        "        extra_levels = num_outs - self.backbone_end_level + self.start_level\n",
        "        # 只有 add_extra_convs 为 True 或 str 时才添加 extra_convs\n",
        "        if self.add_extra_convs and extra_levels >= 1:\n",
        "            for i in range(extra_levels):\n",
        "                if i == 0 and self.add_extra_convs == 'on_input':\n",
        "                    in_channels = self.in_channels[self.backbone_end_level - 1]\n",
        "                else:\n",
        "                    in_channels = out_channels\n",
        "                # extra conv 是3x3步长为2, padding为1的卷积\n",
        "                extra_fpn_conv = ConvModule(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    3,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    conv_cfg=conv_cfg,\n",
        "                    norm_cfg=norm_cfg,\n",
        "                    act_cfg=act_cfg,\n",
        "                    inplace=False)\n",
        "                self.fpn_convs.append(extra_fpn_conv)\n",
        "    # default init_weights for conv(msra) and norm in ConvModule\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize the weights of FPN module.\"\"\"\n",
        "        # 使用xavier初始化卷积层\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                xavier_init(m, distribution='uniform')\n",
        "    \n",
        "    @auto_fp16()\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward function.\"\"\"\n",
        "        assert len(inputs) == len(self.in_channels)\n",
        "\n",
        "        # ====================== 进行水平计算(1x1卷积) ====================\n",
        "        laterals = [\n",
        "            lateral_conv(inputs[i + self.start_level])\n",
        "            for i, lateral_conv in enumerate(self.lateral_convs)\n",
        "        ]\n",
        "        # ==============================================================\n",
        "\n",
        "        # ========================== 计算 top-down =============================\n",
        "        used_backbone_levels = len(laterals)\n",
        "        # 自上至下将 laterals 里面的结果更新为经过 top-down 的结果.\n",
        "        for i in range(used_backbone_levels - 1, 0, -1):\n",
        "           # In some cases, fixing `scale factor` (e.g. 2) is preferred, but\n",
        "            #  it cannot co-exist with `size` in `F.interpolate`.\n",
        "            # 有 scale 的情况\n",
        "            if 'scale_factor' in self.upsample_cfg:\n",
        "                 # 因为range函数不包括右边的端点, 所以可以使用 i - 1\n",
        "                 laterals[i-1] += F.interpolate(laterals[i], **self.upsample_cfg)\n",
        "            # 没有 scale 的情况, 需要计算下层的 feature map 大小.\n",
        "            else:\n",
        "                 # 计算下层 feature map 大小\n",
        "                prev_shape = laterals[i - 1].shape[2:]\n",
        "                laterals[i - 1] += F.interpolate(\n",
        "                    laterals[i], size=prev_shape, **self.upsample_cfg)\n",
        "        # =====================================================================\n",
        "\n",
        "        # ========================== 计算输出的结果 =============================\n",
        "        # part 1: 计算所有 lateral 的输出的结果\n",
        "        outs = [\n",
        "            self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)\n",
        "        ]\n",
        "        \n",
        "        # part 2: 添加 extra levels\n",
        "        if self.num_outs > len(outs):\n",
        "            # 使用 max pool 获得更高层的输出信息, 如: Faster R-CNN, Mask R-CNN (4 lateral + 1 max pool)\n",
        "            if not self.add_extra_convs:\n",
        "                for i in range(self.num_outs - used_backbone_levels):\n",
        "                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n",
        "            # 添加额外的卷积层获得高层输出信息, 如: RetinaNet (3 lateral + 2 conv3x3 stride2)\n",
        "            else:\n",
        "                # 'on_input':   最高层的 feature map 作为 extra 的输入\n",
        "                if self.add_extra_convs == 'on_input':\n",
        "                    extra_source = inputs[self.backbone_end_level - 1]\n",
        "                # 'on_lateral': 最高层的 lateral 结果 作为 extra 的输入\n",
        "                elif self.add_extra_convs == 'on_lateral':\n",
        "                    extra_source = laterals[-1]\n",
        "                # 'on_output':  最高层的经过 conv 的 lateral 结果作为 extra 的输入\n",
        "                elif self.add_extra_convs == 'on_output':\n",
        "                    extra_source = outs[-1]\n",
        "                else:\n",
        "                    raise NotImplementedError\n",
        "                # 计算 input extra\n",
        "                outs.append(self.fpn_convs[used_backbone_levels](extra_source))\n",
        "                # 计算 extra\n",
        "                for i in range(used_backbone_levels + 1, self.num_outs):\n",
        "                    if self.relu_before_extra_convs:\n",
        "                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n",
        "                    else:\n",
        "                        outs.append(self.fpn_convs[i](outs[-1]))\n",
        "        return tuple(outs)\n"
      ],
      "metadata": {
        "id": "710b6nBZbEiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [MMDetection Faster R-CNN 源码详解（三)](https://zhuanlan.zhihu.com/p/184618997)\n",
        "\n"
      ],
      "metadata": {
        "id": "FNzkZiKUcRC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [MMDetection Faster R-CNN 源码详解（四)](https://zhuanlan.zhihu.com/p/194285023)\n",
        "\n",
        "RPN（源码篇）\n",
        "1. 整体结构\n",
        "2. BaseDenseHead\n",
        "4. RPNTestMixin\n",
        "5. RPNHead\n"
      ],
      "metadata": {
        "id": "y64_1Td1cgH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [源码解读：Faster RCNN的细节（一）](https://zhuanlan.zhihu.com/p/65471961)\n",
        "\n",
        "![](https://pica.zhimg.com/v2-2a51ac64bbb4b620ab1aa90f1793b898_1440w.jpg?source=172ae18b)"
      ],
      "metadata": {
        "id": "Kb0DufuYb3os"
      }
    }
  ]
}